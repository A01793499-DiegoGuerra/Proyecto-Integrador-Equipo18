{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8IOl_jVPEU5x",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "import plotly.express as px\n",
    "import random\n",
    "import functools\n",
    "from datetime_truncate import truncate\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from scipy.signal import periodogram\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from deap import base, creator, tools, algorithms\n",
    "from datetime import datetime\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "c1LdQM50EZyH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_magna = pd.read_pickle('base_magna_final.pkl')\n",
    "df_premium = pd.read_pickle('base_premium_final.pkl')\n",
    "df_diesel = pd.read_pickle('base_diesel_final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890
    },
    "id": "dpddBC9wifjb",
    "outputId": "b5874914-5695-44ff-fc1b-c1b1aa1cf057",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stepwise search to minimize aic\n",
      " ARIMA(2,1,2)(0,0,0)[0] intercept   : AIC=inf, Time=23.97 sec\n",
      " ARIMA(0,1,0)(0,0,0)[0] intercept   : AIC=156246.667, Time=0.41 sec\n",
      " ARIMA(1,1,0)(0,0,0)[0] intercept   : AIC=156218.205, Time=0.50 sec\n",
      " ARIMA(0,1,1)(0,0,0)[0] intercept   : AIC=156214.266, Time=1.64 sec\n",
      " ARIMA(0,1,0)(0,0,0)[0]             : AIC=156244.667, Time=0.18 sec\n",
      " ARIMA(1,1,1)(0,0,0)[0] intercept   : AIC=inf, Time=17.61 sec\n",
      " ARIMA(0,1,2)(0,0,0)[0] intercept   : AIC=156172.566, Time=2.41 sec\n",
      " ARIMA(1,1,2)(0,0,0)[0] intercept   : AIC=inf, Time=27.19 sec\n",
      " ARIMA(0,1,3)(0,0,0)[0] intercept   : AIC=156161.174, Time=6.56 sec\n",
      " ARIMA(1,1,3)(0,0,0)[0] intercept   : AIC=inf, Time=30.30 sec\n",
      " ARIMA(0,1,4)(0,0,0)[0] intercept   : AIC=156096.026, Time=8.04 sec\n",
      " ARIMA(1,1,4)(0,0,0)[0] intercept   : AIC=inf, Time=51.98 sec\n",
      " ARIMA(0,1,5)(0,0,0)[0] intercept   : AIC=inf, Time=44.48 sec\n",
      " ARIMA(1,1,5)(0,0,0)[0] intercept   : AIC=inf, Time=59.17 sec\n",
      " ARIMA(0,1,4)(0,0,0)[0]             : AIC=156094.026, Time=3.32 sec\n",
      " ARIMA(0,1,3)(0,0,0)[0]             : AIC=156159.174, Time=1.56 sec\n",
      " ARIMA(1,1,4)(0,0,0)[0]             : AIC=inf, Time=25.57 sec\n",
      " ARIMA(0,1,5)(0,0,0)[0]             : AIC=inf, Time=20.56 sec\n",
      " ARIMA(1,1,3)(0,0,0)[0]             : AIC=inf, Time=12.57 sec\n",
      " ARIMA(1,1,5)(0,0,0)[0]             : AIC=inf, Time=35.45 sec\n",
      "\n",
      "Best model:  ARIMA(0,1,4)(0,0,0)[0]          \n",
      "Total fit time: 373.521 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>SARIMAX Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>y</td>        <th>  No. Observations:  </th>    <td>11664</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>           <td>SARIMAX(0, 1, 4)</td> <th>  Log Likelihood     </th> <td>-78042.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sun, 26 May 2024</td> <th>  AIC                </th> <td>156094.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>06:28:31</td>     <th>  BIC                </th> <td>156130.847</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sample:</th>                  <td>0</td>        <th>  HQIC               </th> <td>156106.395</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th></th>                     <td> - 11664</td>     <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>        <td>opg</td>       <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ma.L1</th>  <td>   -0.0728</td> <td>    0.005</td> <td>  -13.982</td> <td> 0.000</td> <td>   -0.083</td> <td>   -0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ma.L2</th>  <td>   -0.0769</td> <td>    0.008</td> <td>   -9.533</td> <td> 0.000</td> <td>   -0.093</td> <td>   -0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ma.L3</th>  <td>   -0.0555</td> <td>    0.009</td> <td>   -6.127</td> <td> 0.000</td> <td>   -0.073</td> <td>   -0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ma.L4</th>  <td>   -0.0961</td> <td>    0.010</td> <td>  -10.074</td> <td> 0.000</td> <td>   -0.115</td> <td>   -0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sigma2</th> <td> 3.789e+04</td> <td>  262.787</td> <td>  144.169</td> <td> 0.000</td> <td> 3.74e+04</td> <td> 3.84e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Ljung-Box (L1) (Q):</th>     <td>0.67</td> <th>  Jarque-Bera (JB):  </th> <td>18436.43</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Q):</th>                <td>0.41</td> <th>  Prob(JB):          </th>   <td>0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Heteroskedasticity (H):</th> <td>0.44</td> <th>  Skew:              </th>   <td>0.99</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(H) (two-sided):</th>    <td>0.00</td> <th>  Kurtosis:          </th>   <td>8.83</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Covariance matrix calculated using the outer product of gradients (complex-step)."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}          &        y         & \\textbf{  No. Observations:  } &   11664     \\\\\n",
       "\\textbf{Model:}                  & SARIMAX(0, 1, 4) & \\textbf{  Log Likelihood     } & -78042.013  \\\\\n",
       "\\textbf{Date:}                   & Sun, 26 May 2024 & \\textbf{  AIC                } & 156094.026  \\\\\n",
       "\\textbf{Time:}                   &     06:28:31     & \\textbf{  BIC                } & 156130.847  \\\\\n",
       "\\textbf{Sample:}                 &        0         & \\textbf{  HQIC               } & 156106.395  \\\\\n",
       "\\textbf{}                        &      - 11664     & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}        &       opg        & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{ma.L1}  &      -0.0728  &        0.005     &   -13.982  &         0.000        &       -0.083    &       -0.063     \\\\\n",
       "\\textbf{ma.L2}  &      -0.0769  &        0.008     &    -9.533  &         0.000        &       -0.093    &       -0.061     \\\\\n",
       "\\textbf{ma.L3}  &      -0.0555  &        0.009     &    -6.127  &         0.000        &       -0.073    &       -0.038     \\\\\n",
       "\\textbf{ma.L4}  &      -0.0961  &        0.010     &   -10.074  &         0.000        &       -0.115    &       -0.077     \\\\\n",
       "\\textbf{sigma2} &    3.789e+04  &      262.787     &   144.169  &         0.000        &     3.74e+04    &     3.84e+04     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Ljung-Box (L1) (Q):}     & 0.67 & \\textbf{  Jarque-Bera (JB):  } & 18436.43  \\\\\n",
       "\\textbf{Prob(Q):}                & 0.41 & \\textbf{  Prob(JB):          } &   0.00    \\\\\n",
       "\\textbf{Heteroskedasticity (H):} & 0.44 & \\textbf{  Skew:              } &   0.99    \\\\\n",
       "\\textbf{Prob(H) (two-sided):}    & 0.00 & \\textbf{  Kurtosis:          } &   8.83    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{SARIMAX Results}\n",
       "\\end{center}\n",
       "\n",
       "Warnings: \\newline\n",
       " [1] Covariance matrix calculated using the outer product of gradients (complex-step)."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                               SARIMAX Results                                \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                11664\n",
       "Model:               SARIMAX(0, 1, 4)   Log Likelihood              -78042.013\n",
       "Date:                Sun, 26 May 2024   AIC                         156094.026\n",
       "Time:                        06:28:31   BIC                         156130.847\n",
       "Sample:                             0   HQIC                        156106.395\n",
       "                              - 11664                                         \n",
       "Covariance Type:                  opg                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "ma.L1         -0.0728      0.005    -13.982      0.000      -0.083      -0.063\n",
       "ma.L2         -0.0769      0.008     -9.533      0.000      -0.093      -0.061\n",
       "ma.L3         -0.0555      0.009     -6.127      0.000      -0.073      -0.038\n",
       "ma.L4         -0.0961      0.010    -10.074      0.000      -0.115      -0.077\n",
       "sigma2      3.789e+04    262.787    144.169      0.000    3.74e+04    3.84e+04\n",
       "===================================================================================\n",
       "Ljung-Box (L1) (Q):                   0.67   Jarque-Bera (JB):             18436.43\n",
       "Prob(Q):                              0.41   Prob(JB):                         0.00\n",
       "Heteroskedasticity (H):               0.44   Skew:                             0.99\n",
       "Prob(H) (two-sided):                  0.00   Kurtosis:                         8.83\n",
       "===================================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculando modelo ARIMA\n",
    "model_auto_arima = auto_arima(df_magna['volumen_despachado'], seasonal=True, trace=True)\n",
    "model_auto_arima.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZpK7_JZkUJm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Los valores AIC para el modelo óptimo ARIMA de volumen despachado son demasiado altos (oscilan en torno a 150 mil). De esta manera, un modelo ARIMA no seria el adecuado para este tipo de datos de volumen despachado MAGNA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "MzYyOylbEcuR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_unix_time(date):\n",
    "    return int((date - datetime(1970, 1, 1)).total_seconds())\n",
    "\n",
    "\n",
    "def feature_engineering(df_interest):\n",
    "    df_interest['hour_sin'] = np.sin(2 * np.pi * df_interest['hour']/23.0)\n",
    "    df_interest['hour_cos'] = np.cos(2 * np.pi * df_interest['hour']/23.0)\n",
    "    df_interest['day_sin'] = np.sin(2 * np.pi * df_interest['dow_n']/6.0)\n",
    "    df_interest['day_cos'] = np.cos(2 * np.pi * df_interest['dow_n']/6.0)\n",
    "    df_interest['month_sin'] = np.sin(2 * np.pi * df_interest['month']/11.0)\n",
    "    df_interest['month_cos'] = np.cos(2 * np.pi * df_interest['month']/11.0)\n",
    "    df_interest['unix_time'] = df_interest['sale_timeStamp'].apply(convert_to_unix_time)\n",
    "    df_interest.drop(['sale_timeStamp'], axis = 1, inplace = True)\n",
    "    return df_interest\n",
    "\n",
    "\n",
    "def setup_models(df_interest, model_features, model_target):\n",
    "    alt_df = feature_engineering(df_interest[model_features].copy())\n",
    "    target_df = df_interest[model_target].copy()\n",
    "\n",
    "    new_df = pd.concat([alt_df, target_df], axis=1)\n",
    "\n",
    "    train_percentage = 0.7\n",
    "    slice_point = int(new_df.shape[0] * train_percentage)\n",
    "\n",
    "    train_df = new_df.iloc[:slice_point]\n",
    "    test_df = new_df.iloc[slice_point:]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train = scaler.fit_transform(train_df[alt_df.columns])\n",
    "    X_test = scaler.transform(test_df[alt_df.columns])\n",
    "\n",
    "    y_train = train_df[model_target].values.ravel()\n",
    "    y_test = test_df[model_target].values.ravel()\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred, force_finite=False)\n",
    "    print(f'Demand Prediction Model MSE: {mse}, R^2: {r2}')\n",
    "    return model, X_train, scaler, df_interest['precio_neto'].mean(), alt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "dcmTh42GEgDi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pe_features = ['volumen_despachado', 'precio_neto']\n",
    "\n",
    "\n",
    "def pricing_elasticity_prep(df_interest, features):\n",
    "    new_df = df_interest[features].copy()\n",
    "    type_dict = {}\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        type_dict[i] = df_interest[feature].dtype\n",
    "\n",
    "    new_df.replace(0, pd.NA, inplace=True)\n",
    "    new_df.dropna(inplace=True)\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        new_df[feature] = new_df[feature].astype(type_dict[i])\n",
    "\n",
    "    new_df['log_price'] = np.log(new_df['precio_neto'])\n",
    "    new_df['log_demand'] = np.log(new_df['volumen_despachado'])\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "mSME4WeBEjC6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_magna_alt = pricing_elasticity_prep(df_magna, pe_features)\n",
    "df_premium_alt = pricing_elasticity_prep(df_premium, pe_features)\n",
    "df_diesel_alt = pricing_elasticity_prep(df_diesel, pe_features)\n",
    "\n",
    "\n",
    "model_lr_elasticity_magna = LinearRegression()\n",
    "model_lr_elasticity_magna.fit(df_magna_alt[['log_price']], df_magna_alt[['log_demand']])\n",
    "price_elasticity_magna = model_lr_elasticity_magna.coef_[0]\n",
    "\n",
    "model_lr_elasticity_premium = LinearRegression()\n",
    "model_lr_elasticity_premium.fit(df_premium_alt[['log_price']], df_premium_alt[['log_demand']])\n",
    "price_elasticity_premium = model_lr_elasticity_premium.coef_[0]\n",
    "\n",
    "model_lr_elasticity_diesel = LinearRegression()\n",
    "model_lr_elasticity_diesel.fit(df_diesel_alt[['log_price']], df_diesel_alt[['log_demand']])\n",
    "price_elasticity_diesel = model_lr_elasticity_diesel.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fcPLxxdMSJk",
    "outputId": "a9151340-ccea-4c3a-d725-ba414836279f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demand Prediction Model MSE: 16544.637730899, R^2: 0.7121078523185419\n",
      "Máximo global en x = [17.24642578] , rentabilidad = 2075.724777670202\n",
      "Optimal price: $17.25\n",
      "Maximum Gross Margin: $2075.72\n"
     ]
    }
   ],
   "source": [
    "#Modelo Evolucion Diferencial (para cada base de datos)\n",
    "#Aqui estuve modificando el penalty y en 10000 dio algo coherente con nuestros datos\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def feature_engineering(df_interest):\n",
    "    df_interest['hour_sin'] = np.sin(2 * np.pi * df_interest['hour'] / 23.0)\n",
    "    df_interest['hour_cos'] = np.cos(2 * np.pi * df_interest['hour'] / 23.0)\n",
    "    df_interest['day_sin'] = np.sin(2 * np.pi * df_interest['dow_n'] / 6.0)\n",
    "    df_interest['day_cos'] = np.cos(2 * np.pi * df_interest['dow_n'] / 6.0)\n",
    "    df_interest['month_sin'] = np.sin(2 * np.pi * df_interest['month'] / 11.0)\n",
    "    df_interest['month_cos'] = np.cos(2 * np.pi * df_interest['month'] / 11.0)\n",
    "    df_interest['unix_time'] = df_interest['sale_timeStamp'].apply(lambda x: x.timestamp())\n",
    "    df_interest.drop(['sale_timeStamp'], axis=1, inplace=True)\n",
    "    return df_interest\n",
    "\n",
    "def setup_models(df_interest, model_features, model_target):\n",
    "    alt_df = feature_engineering(df_interest[model_features].copy())\n",
    "    target_df = df_interest[model_target].copy()\n",
    "\n",
    "    new_df = pd.concat([alt_df, target_df], axis=1)\n",
    "\n",
    "    train_percentage = 0.7\n",
    "    slice_point = int(new_df.shape[0] * train_percentage)\n",
    "\n",
    "    train_df = new_df.iloc[:slice_point]\n",
    "    test_df = new_df.iloc[slice_point:]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train = scaler.fit_transform(train_df[alt_df.columns])\n",
    "    X_test = scaler.transform(test_df[alt_df.columns])\n",
    "\n",
    "    y_train = train_df[model_target].values.ravel()\n",
    "    y_test = test_df[model_target].values.ravel()\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred, force_finite=False)\n",
    "    print(f'Demand Prediction Model MSE: {mse}, R^2: {r2}')\n",
    "    return model, X_train, scaler, df_interest['precio_neto'].mean(), alt_df\n",
    "\n",
    "def eval_gross_margin(individual, train_data, model, scaler, current_price, elasticity, feature_names):\n",
    "    try:\n",
    "        price = individual[0]\n",
    "        mean_values = np.mean(train_data[:, 1:], axis=0)\n",
    "        data_with_price = np.insert(mean_values, 0, price)\n",
    "        temp_features = pd.DataFrame([data_with_price], columns=feature_names)\n",
    "\n",
    "        scaled_features = scaler.transform(temp_features)\n",
    "        predicted_demand = model.predict(scaled_features)[0]\n",
    "\n",
    "        cost_of_goods_sold = temp_features['costo_neto'].iloc[0]\n",
    "\n",
    "        gross_margin = (price - cost_of_goods_sold) * predicted_demand\n",
    "        penalty_factor = 10000  # Incrementar la penalización\n",
    "        penalty = penalty_factor * (abs(price - current_price) / current_price) ** (-elasticity)\n",
    "        gross_margin -= penalty\n",
    "        return (gross_margin,)\n",
    "    except Exception as e:\n",
    "        print(\"Error in evalGrossMargin:\", e)\n",
    "        return (0,)\n",
    "\n",
    "def differential_evolution(df_interest, model_features, model_target, pe_features, lower_bound_ratio, upper_bound_ratio):\n",
    "    model, X_train, scaler, current_price, features_refined = setup_models(df_interest, model_features, model_target)\n",
    "    elasticity = -0.69\n",
    "\n",
    "    lower_bound = current_price * lower_bound_ratio\n",
    "    upper_bound = current_price * upper_bound_ratio\n",
    "\n",
    "    xl = np.array([lower_bound])\n",
    "    xu = np.array([upper_bound])\n",
    "\n",
    "    G = 30  # Número de generaciones\n",
    "    N = 50  # Tamaño de la población\n",
    "    D = 1  # Dimensión de la búsqueda (solo precio)\n",
    "\n",
    "    F = 0.6  # Factor de escala para la mutación\n",
    "    CR = 0.9  # Tasa de recombinación\n",
    "\n",
    "    x = np.zeros((D, N))\n",
    "    fitness = np.zeros(N)\n",
    "\n",
    "    for i in range(N):\n",
    "        x[:, i] = xl + (xu - xl) * np.random.rand(D)\n",
    "        fitness[i] = eval_gross_margin(x[:, i], X_train, model, scaler, current_price, elasticity, features_refined.columns)[0]\n",
    "\n",
    "    fx_plot = np.zeros(G)\n",
    "\n",
    "    for n in range(G):\n",
    "        for i in range(N):\n",
    "            r1 = i\n",
    "            while r1 == i:\n",
    "                r1 = np.random.randint(N)\n",
    "\n",
    "            r2 = r1\n",
    "            while r2 == r1 or r2 == i:\n",
    "                r2 = np.random.randint(N)\n",
    "\n",
    "            r3 = r2\n",
    "            while r3 == r2 or r3 == r1 or r3 == i:\n",
    "                r3 = np.random.randint(N)\n",
    "\n",
    "            v = x[:, r1] + F * (x[:, r2] - x[:, r3])\n",
    "\n",
    "            u = np.zeros(D)\n",
    "\n",
    "            for j in range(D):\n",
    "                r = np.random.rand()\n",
    "\n",
    "                if r <= CR:\n",
    "                    u[j] = v[j]\n",
    "                else:\n",
    "                    u[j] = x[j, i]\n",
    "\n",
    "            fitness_u = eval_gross_margin(u, X_train, model, scaler, current_price, elasticity, features_refined.columns)[0]\n",
    "\n",
    "            if fitness_u > fitness[i]:\n",
    "                x[:, i] = u\n",
    "                fitness[i] = fitness_u\n",
    "\n",
    "        fx_plot[n] = np.max(fitness)\n",
    "\n",
    "    igb = np.argmax(fitness)\n",
    "\n",
    "    print(\"Máximo global en x =\", x[:, igb], \", rentabilidad =\", fitness[igb])\n",
    "    return x[:, igb], fitness[igb]\n",
    "\n",
    "pe_features = ['volumen_despachado', 'precio_neto']\n",
    "\n",
    "model_features = ['precio_neto', 'costo_neto', 'precio_brent', 'hour', 'month', 'dow_n','sale_timeStamp']\n",
    "model_target = ['volumen_despachado']\n",
    "\n",
    "optimal_price, max_gross_margin = differential_evolution(df_magna, model_features, model_target, pe_features, 0.7, 1.3)\n",
    "print(f'Optimal price: ${optimal_price[0]:.2f}')\n",
    "print(f'Maximum Gross Margin: ${max_gross_margin:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JUAZZfmWExOK",
    "outputId": "e2dc8881-4efa-4e25-cdff-39ad5ae96a27",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demand Prediction Model MSE: 16544.637730899, R^2: 0.7121078523185419\n",
      "differential_evolution step 1: f(x)= -2653.72\n",
      "differential_evolution step 2: f(x)= -2654.46\n",
      "differential_evolution step 3: f(x)= -2654.46\n",
      "differential_evolution step 4: f(x)= -2654.5\n",
      "differential_evolution step 5: f(x)= -2654.5\n",
      "differential_evolution step 6: f(x)= -2654.5\n",
      "differential_evolution step 7: f(x)= -2654.5\n",
      "differential_evolution step 8: f(x)= -2654.69\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "Optimal price: $22.42\n",
      "Maximum Gross Margin: $2654.72\n",
      "Demand Prediction Model MSE: 2254.319061998398, R^2: 0.4500814085871191\n",
      "differential_evolution step 1: f(x)= -80.6669\n",
      "differential_evolution step 2: f(x)= -80.6669\n",
      "differential_evolution step 3: f(x)= -80.6669\n",
      "differential_evolution step 4: f(x)= -81.0441\n",
      "differential_evolution step 5: f(x)= -81.0762\n",
      "differential_evolution step 6: f(x)= -81.0974\n",
      "differential_evolution step 7: f(x)= -81.0974\n",
      "differential_evolution step 8: f(x)= -81.148\n",
      "differential_evolution step 9: f(x)= -81.148\n",
      "differential_evolution step 10: f(x)= -81.148\n",
      "differential_evolution step 11: f(x)= -81.158\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "Optimal price: $25.68\n",
      "Maximum Gross Margin: $81.16\n",
      "Demand Prediction Model MSE: 761.5569677562796, R^2: 0.09935912415208636\n",
      "differential_evolution step 1: f(x)= -2372.26\n",
      "differential_evolution step 2: f(x)= -2380.58\n",
      "differential_evolution step 3: f(x)= -2380.58\n",
      "differential_evolution step 4: f(x)= -2380.94\n",
      "differential_evolution step 5: f(x)= -2381.07\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "Optimal price: $25.29\n",
      "Maximum Gross Margin: $2381.16\n"
     ]
    }
   ],
   "source": [
    "#Modelo cuando pedi que ChatGPT lo haga para cada base de datos, incluyo nuevas funciones y librerias\n",
    "#sin embargo, el modelo se crea en la parte de arriba\n",
    "#Es decir, utiliza la arquitectura que armo Hans con el Toolbox pero implementando el modelo de evolucion diferencial\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Feature engineering\n",
    "def feature_engineering(df_interest):\n",
    "    df_interest['hour_sin'] = np.sin(2 * np.pi * df_interest['hour']/23.0)\n",
    "    df_interest['hour_cos'] = np.cos(2 * np.pi * df_interest['hour']/23.0)\n",
    "    df_interest['day_sin'] = np.sin(2 * np.pi * df_interest['dow_n']/6.0)\n",
    "    df_interest['day_cos'] = np.cos(2 * np.pi * df_interest['dow_n']/6.0)\n",
    "    df_interest['month_sin'] = np.sin(2 * np.pi * df_interest['month']/11.0)\n",
    "    df_interest['month_cos'] = np.cos(2 * np.pi * df_interest['month']/11.0)\n",
    "    df_interest['unix_time'] = df_interest['sale_timeStamp'].apply(lambda x: pd.to_datetime(x).timestamp())\n",
    "    df_interest.drop(['sale_timeStamp'], axis = 1, inplace = True)\n",
    "    return df_interest\n",
    "\n",
    "# Setup models\n",
    "def setup_models(df_interest, features, target):\n",
    "    alt_df = feature_engineering(df_interest[features].copy())\n",
    "    target_df = df_interest[target].copy()\n",
    "\n",
    "    new_df = pd.concat([alt_df, target_df], axis = 1)\n",
    "\n",
    "    train_percentage = 0.7\n",
    "    slice_point = int(new_df.shape[0] * train_percentage)\n",
    "\n",
    "    train_df = new_df.iloc[:slice_point]\n",
    "    test_df = new_df.iloc[slice_point:]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train = scaler.fit_transform(train_df[alt_df.columns])\n",
    "    X_test = scaler.transform(test_df[alt_df.columns])\n",
    "\n",
    "    y_train = train_df[target].values.ravel()\n",
    "    y_test = test_df[target].values.ravel()\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred, force_finite = False)\n",
    "    print(f'Demand Prediction Model MSE: {mse}, R^2: {r2}')\n",
    "    return model, X_train, scaler, df_interest['precio_neto'].mean(), alt_df\n",
    "\n",
    "# Evaluate gross margin\n",
    "def eval_gross_margin(price, train_data, model, scaler, current_price, elasticity, feature_names, penalty_factor=100):\n",
    "    try:\n",
    "        price = price[0]\n",
    "        # Create a dataframe to simulate the scenario with the evaluated price\n",
    "        mean_values = np.mean(train_data[:, 1:], axis=0)\n",
    "        data_with_price = np.insert(mean_values, 0, price)\n",
    "        temp_features = pd.DataFrame([data_with_price], columns = feature_names)\n",
    "\n",
    "        # Calculate the scaled features for the model\n",
    "        scaled_features = scaler.transform(temp_features)\n",
    "        predicted_demand = model.predict(scaled_features)[0]\n",
    "\n",
    "        # Extract cost of goods sold from the temporary features, ensuring it corresponds to the simulated scenario\n",
    "        cost_of_goods_sold = temp_features['costo_neto'].iloc[0]\n",
    "\n",
    "        gross_margin = (price - cost_of_goods_sold) * predicted_demand\n",
    "        penalty = penalty_factor * (abs(price - current_price) / current_price) ** (-elasticity)\n",
    "        gross_margin -= penalty\n",
    "        return -gross_margin  # Return negative because differential_evolution minimizes the function\n",
    "    except Exception as e:\n",
    "        print(\"Error in eval_gross_margin:\", e)\n",
    "        return 0  # Return a default or zero fitness in case of error\n",
    "\n",
    "# Optimize price using Differential Evolution\n",
    "def optimize_price(df, features, target, elasticity, lower_bound_ratio, upper_bound_ratio, penalty_factor):\n",
    "    model, X_train, scaler, current_price, features_refined = setup_models(df, features, target)\n",
    "\n",
    "    lower_bound = current_price * lower_bound_ratio\n",
    "    upper_bound = current_price * upper_bound_ratio\n",
    "\n",
    "    bounds = [(lower_bound, upper_bound)]\n",
    "\n",
    "    result = differential_evolution(eval_gross_margin, bounds,\n",
    "                                    args=(X_train, model, scaler, current_price, elasticity, features_refined.columns, penalty_factor),\n",
    "                                    strategy='best1bin', maxiter=1000, popsize=50, tol=0.01, mutation=(0.5, 1), recombination=0.7, disp=True)\n",
    "\n",
    "    optimal_price = result.x[0]\n",
    "    max_gross_margin = -result.fun\n",
    "\n",
    "    print(f'Optimal price: ${optimal_price:.2f}')\n",
    "    print(f'Maximum Gross Margin: ${max_gross_margin:.2f}')\n",
    "\n",
    "    return optimal_price, max_gross_margin\n",
    "\n",
    "# Example usage\n",
    "model_features = ['precio_neto', 'costo_neto', 'precio_brent', 'hour', 'month', 'dow_n','sale_timeStamp']\n",
    "model_target = ['volumen_despachado']\n",
    "\n",
    "elasticity_magna = price_elasticity_magna\n",
    "elasticity_premium = price_elasticity_premium\n",
    "elasticity_diesel = price_elasticity_diesel\n",
    "\n",
    "lower_bound_ratio = 0.7\n",
    "upper_bound_ratio = 1.3\n",
    "penalty_factor = 100\n",
    "\n",
    "magna_price, magna_gross_margin = optimize_price(df_magna, model_features, model_target, elasticity_magna, lower_bound_ratio, upper_bound_ratio, penalty_factor)\n",
    "premium_price, premium_gross_margin = optimize_price(df_premium, model_features, model_target, elasticity_premium, lower_bound_ratio, upper_bound_ratio, penalty_factor)\n",
    "diesel_price, diesel_gross_margin = optimize_price(df_diesel, model_features, model_target, elasticity_diesel, lower_bound_ratio, upper_bound_ratio, penalty_factor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LAeiUSaFqE2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
