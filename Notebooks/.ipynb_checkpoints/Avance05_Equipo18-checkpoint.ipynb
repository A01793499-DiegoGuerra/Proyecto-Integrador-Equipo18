{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TC5035.10 Proyecto Integrador\n",
    " \n",
    "Dra. Grettel Barceló Alonso\n",
    "Dr. Luis Eduardo Falcón Morales\n",
    "\n",
    "Liga Github: https://github.com/A01793499-DiegoGuerra/Proyecto-Integrador-Equipo18/tree/main\n",
    "\n",
    "#### Equipo 18 : “Modelos para la Optimización de Precios en Estaciones de Autoservicio”\n",
    "\n",
    "* Diego Fernando Guerra Burgos\tA01793499\n",
    "* Esteban Sánchez Retamoza\t\tA01740631\n",
    "* Hansel Zapiain Rodríguez\t\tA00469031"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avance 1. Análisis exploratorio de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   \n",
    "Abril 2024\n",
    "\n",
    "**Objetivos**\n",
    "\n",
    "2.1 Elegir las características más relevantes para reducir la dimensionalidad y aumentar la capacidad de generalización del modelo.\n",
    "\n",
    "2.2 Abordar y corregir los problemas identificados en los datos.\n",
    "\n",
    "**Instrucciones**\n",
    "\n",
    "Este primer avance consiste en realizar un análisis exploratorio de datos (EDA - Exploratory Data Analysis), es decir, describir los datos utilizando técnicas estadísticas y de visualización (análisis univariante y bi/multivariante) para hacer enfoque en sus aspectos más relevantes, así como aplicar y justificar operaciones de preprocesamiento, relacionadas con el manejo de valores faltantes, atípicos y alta cardinalidad. Es importante que incluyan sus conclusiones del EDA, identificando tendencias o relaciones importantes.\n",
    "\n",
    "Las siguientes son algunas de las preguntas comunes que podrán abordar a través del EDA:\n",
    "\n",
    "¿Hay valores faltantes en el conjunto de datos? ¿Se pueden identificar patrones de ausencia? \n",
    "¿Cuáles son las estadísticas resumidas del conjunto de datos?\n",
    "¿Hay valores atípicos en el conjunto de datos?\n",
    "¿Cuál es la cardinalidad de las variables categóricas?\n",
    "¿Existen distribuciones sesgadas en el conjunto de datos? ¿Necesitamos aplicar alguna transformación no lineal?\n",
    "¿Se identifican tendencias temporales? (En caso de que el conjunto incluya una dimensión de tiempo).\n",
    "¿Hay correlación entre las variables dependientes e independientes?\n",
    "¿Cómo se distribuyen los datos en función de diferentes categorías?\n",
    "¿Existen patrones o agrupaciones (clusters) en los datos con características similares?\n",
    "¿Se deberían normalizar las imágenes para visualizarlas mejor?\n",
    "¿Hay desequilibrio en las clases de la variable objetivo?\n",
    "Deberán contar con un repositorio en GitHubLinks to an external site., para compartir los resultados con el equipo docente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uta30eJkwMkn"
   },
   "source": [
    "***Importar Librerias***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Uta30eJkwMkn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "import plotly.express as px\n",
    "import random\n",
    "import functools\n",
    "\n",
    "from datetime_truncate import truncate\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "from datetime import datetime\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "\n",
    "from scipy.signal import periodogram\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "from pyswarm import pso\n",
    "from tabulate import tabulate\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from deap import base, creator, tools, algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IM6BKQlYHzZ7"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '99_Datasets/'# this needs to be changed to the directory of the excel files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMZNNul3pFT5"
   },
   "source": [
    "***Consolidación de datos (Opcional)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-EypGPWR3gyn",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '99_Datasets/Compras 2022 V2.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m df_cost \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m cost_files_db:\n\u001b[1;32m----> 5\u001b[0m     year_db_cost \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(file, skiprows \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      6\u001b[0m     df_cost \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_cost, year_db_cost], ignore_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m df_cost\u001b[38;5;241m.\u001b[39mto_pickle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mbase_compras_combinada_v2.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m ExcelFile(\n\u001b[0;32m    496\u001b[0m         io,\n\u001b[0;32m    497\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    498\u001b[0m         engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[0;32m    499\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[0;32m    500\u001b[0m     )\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m inspect_excel_format(\n\u001b[0;32m   1551\u001b[0m         content_or_path\u001b[38;5;241m=\u001b[39mpath_or_buffer, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[0;32m   1552\u001b[0m     )\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m   1403\u001b[0m     content_or_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1404\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '99_Datasets/Compras 2022 V2.xlsx'"
     ]
    }
   ],
   "source": [
    "cost_files_db = [f'{path}Compras 2022 V2.xlsx', f'{path}Compras 2023 V2.xlsx', f'{path}Compras 2024 V2.xlsx']\n",
    "df_cost = pd.DataFrame()\n",
    "\n",
    "for file in cost_files_db:\n",
    "    year_db_cost = pd.read_excel(file, skiprows = 4)\n",
    "    df_cost = pd.concat([df_cost, year_db_cost], ignore_index = True)\n",
    "\n",
    "df_cost.to_pickle(f'{path}base_compras_combinada_v2.pkl')\n",
    "\n",
    "\n",
    "sales_files_db = [f'{path}Ventas 2022 V2.xlsx', f'{path}Ventas 2023 V2.xlsx', f'{path}Ventas 2024 V2.xlsx']\n",
    "df_sales = pd.DataFrame()\n",
    "\n",
    "for file in sales_files_db:\n",
    "    year_db_sales = pd.read_excel(file, skiprows = 4)\n",
    "    df_sales = pd.concat([df_sales, year_db_sales], ignore_index = True)\n",
    "\n",
    "df_sales.to_pickle(f'{path}base_ventas_combinada_v2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Carga de Bases***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este proyecto tenemos 2 fuentes de información primarias que se obtuvieron directamente de los sistemas ERP del Grupo Golden\n",
    "\n",
    "**Transacciones de Ventas**: Considera todos los despachos ejecutados en una gasolinera de grupo golden, incluyendo la distinción de volumenes de compra así como datos adicionales.\n",
    "\n",
    "**Transacciones de Compra**: Considera todas las operaciones de compra de combustible para reabastecer a la gasolinera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "gnxMbCC93mxm",
    "outputId": "4a369161-4209-4aac-923f-5060e7f9af16"
   },
   "outputs": [],
   "source": [
    "df_sales = pd.read_pickle(f'{path}base_ventas_combinada_v2.pkl')\n",
    "df_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_purchases = pd.read_pickle(f'{path}base_compras_combinada_v2.pkl')\n",
    "df_purchases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Información Básica Fuentes de Información***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disponemos de una base de ventas de 15 variables. También tenemos dos variables (Fecha y Hora) las cuales deben ser cambiadas a formato DATETIME para poder analizar componentes temporales. En cuanto a la variable Posición, que es una variable categórica y tiene formato de una variable numérica, debe ser cambiada para reflejar el verdadero tipo de dato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.rename(columns = {'Folio':'folio_venta', 'Precio de Venta del litro con impuestos':'precio_bruto', 'Precio de Venta del litro sin impuestos':'precio_neto', 'Venta Ticket (con impuestos)':'venta_bruta', 'Venta sin impuestos':'venta_neta', 'Venta Unidades':'volumen_despachado', 'Producto':'producto'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disponemos de una base de compras de 14 variables. También tenemos dos variables (Fecha y Hora) las cuales deben ser cambiadas a formato DATETIME para poder unir ambas bases. De la misma manera tenemos datos a nivel factura asi como nivel unitarios. Para determinar el margen bruto de cada operación de despacho, nuestro interes es el costo unitario del combustible por lo que los campos IVA F, IEPS F, Sin Imp F y Precio Factura los descartaremos del data frame más adelante. En cuanto a la variable Tanque, que es una variable categórica, debe ser eliminada del análisis dado que solo es un indicador de donde fue depositado el combustible por lo que no agregara valor al análisis más adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_purchases.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_purchases.drop(['Precio Factura', 'IVA F', 'IEPS F', 'Sin imp F', 'Tanque'], axis = 1, inplace = True)\n",
    "df_purchases.rename(columns = {'Folio':'folio_compra','Por litro U':'costo_bruto', 'Sin imp U':'costo_neto', 'Producto':'producto'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Transformación de Bases***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para consolidar las bases de datos es importante garantizar que todos los campos de fecha existentes sean los correctos. En los siguientes pasos se crearan las estampas de tiempo que nos permitiran hacer este proceso, de la misma manera nos garantizará que podamos hacer análisis de temporalidad hacia adelante. De la misma manera se eliminaran los campos de Fecha y Hora dado que todas las operaciones hacia adelante se buscaran hacer con la estampa de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales['sale_date'] = pd.to_datetime(df_sales['Fecha'], errors = 'coerce', infer_datetime_format = True)\n",
    "df_sales['sale_localtime'] = pd.to_timedelta(df_sales['Hora'].astype(str))\n",
    "df_sales['sale_timeStamp'] = df_sales['sale_date'] + df_sales['sale_localtime']\n",
    "\n",
    "df_sales.drop(['Fecha', 'Hora'], axis = 1, inplace = True)\n",
    "df_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_purchases['purchase_date'] = pd.to_datetime(df_purchases['Fecha'], errors = 'coerce', infer_datetime_format = True)\n",
    "df_purchases['purchase_localtime'] = pd.to_timedelta(df_purchases['Hora'].astype(str))\n",
    "df_purchases['purchase_timeStamp'] = df_purchases['purchase_date'] + df_purchases['purchase_localtime']\n",
    "\n",
    "df_purchases.drop(['Fecha', 'Hora'], axis = 1, inplace = True)\n",
    "df_purchases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXQr9fPWo3AL"
   },
   "source": [
    "***Información datos faltantes o NA***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, los datos de venta principales como la fecha de transacción, cantidad de combustible, tipo de combustible, precio, entre otros, no registran valores faltantes. La gran mayoría de datos faltantes se observan en información del cliente como tipo de vehículo, el nombre, placas, etc. En este caso, los patrones de ausencia se deben a la falta de recolección o estandarización de este tipo de datos por parte de la gasolinera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculando data faltante en la base de datos por columna\n",
    "missing_percentage_per_column_sales = (df_sales.isnull().mean() * 100).round(2)\n",
    "print(\"Presencia de datos faltantes por dimensión (en porcentaje):\")\n",
    "print(missing_percentage_per_column_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, los datos de compras principales como la fecha de transacción, cantidad de combustible, tipo de combustible, precio, entre otros, no registran valores faltantes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculando data faltante en la base de datos por columna\n",
    "missing_percentage_per_column_purchases = (df_purchases.isnull().mean() * 100).round(2)\n",
    "print(\"Presencia de datos faltantes por dimensión (en porcentaje):\")\n",
    "print(missing_percentage_per_column_purchases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Unificación de Bases***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando las estampas de tiempo obtendremos el costo de reposición de cada despacho a cliente. Esto nos permitirá obtener un cálculo de margen bruto que al finalizar estará atado con nuestro objetivo final de optimización de precios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.sort_values(['producto', 'sale_date'], inplace = True)\n",
    "df_purchases.sort_values(['producto', 'purchase_date'], inplace = True)\n",
    "\n",
    "df_master = pd.DataFrame()\n",
    "\n",
    "for product_type in df_sales['producto'].unique():\n",
    "\n",
    "    sales_temp = df_sales[df_sales['producto'] == product_type]\n",
    "    purchases_temp = df_purchases[df_purchases['producto'] == product_type]\n",
    "    \n",
    "    merged_temp = pd.merge_asof(sales_temp, purchases_temp, left_on = 'sale_date', right_on = 'purchase_date',\n",
    "                                by = 'producto', direction = 'backward')\n",
    "\n",
    "    # Append the result to the main DataFrame\n",
    "    df_master = pd.concat([df_master, merged_temp], ignore_index = True)\n",
    "\n",
    "df_master.head(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Información Básica Data Frame***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSP6Fxa_pjb6"
   },
   "source": [
    "***Estadísticas de las variables***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sxehTS5RqPcN",
    "outputId": "5cc170ee-1e1a-450d-aaba-418e5d946d2b"
   },
   "outputs": [],
   "source": [
    "unique_values_per_column = df_master.nunique()\n",
    "unique_values_per_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hAJ2X4T6q-Qa",
    "outputId": "184708d8-9392-4a64-862d-1f9696e27130"
   },
   "outputs": [],
   "source": [
    "print('Identificación de manguera dispensadora:')\n",
    "print(df_master['Posición'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fSJzFMtV1aYc",
    "outputId": "f123a071-79c7-4d44-a5dc-67508a768a0b"
   },
   "outputs": [],
   "source": [
    "print('Tipo de Combustible vendido:')\n",
    "print(df_master['producto'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Afg8L7Q_sSCg"
   },
   "source": [
    "***Presencia de valores atípicos***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestras variables numéricas son Volumen, Precio Neto, Precio Bruto, Costo Neto, Costo Bruto, Venta Bruta. En estos gráficos de Caja y Densidad podemos observar la presencia de valores atípicos en Cantidad e Importe. Estos datos atípicos reflejan valores demasiado altos, lo cual resulta en que exista una distribución sesgada a la izquierda. Se debería eliminar estos registros para observar la nueva distribución.\n",
    "\n",
    "En el caso de Precio, podemos observar que existe una distribución ligeramente sesgada a la izquierda, con la presencia de pocos valores atípicos, pero que se encuentran en valores lógicos, a comparación de los valores atípicos en las otras variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jzmy_VEKsQTz",
    "outputId": "6c370e28-3937-4e9a-b777-70c3ba9ff257"
   },
   "outputs": [],
   "source": [
    "columnas_numericas = ['volumen_despachado', 'precio_neto', 'precio_bruto', 'costo_neto', 'costo_bruto','venta_bruta']\n",
    "\n",
    "for column in columnas_numericas:\n",
    "    plt.figure()\n",
    "    df_master[column].plot(kind = 'box')\n",
    "    plt.title(f'Gráfico de caja de {column}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "gldkAL0lxePL",
    "outputId": "444aba98-5a5e-4019-a2e8-464c9a9c172d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "\n",
    "for i, columna in enumerate(columnas_numericas, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    df_master[columna].plot(kind = 'density', color = 'skyblue')\n",
    "    plt.xlabel('Valor')\n",
    "    plt.ylabel('Densidad')\n",
    "    plt.title(f'Gráfico de Densidad de {columna}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tratamiento de valores atípicos***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZ4DYg-2x06o"
   },
   "source": [
    "En este caso podemos observar que existen valores atípicos que tal vez son producto de mal registro por parte de la Gasolinera, lo que resulta en valores demasiado altos. Por ejemplo, el valor máximo de *Cantidad* se ubica en 6 millones de galones por una sola venta (un vehículo promedio se llena con 12 galones). Por lo cual para filtrar estos datos atípicos, vamos a eliminar, solamente para este análisis exploratorio, el 10% de los datos más altos de la serie de *Cantidad*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "YtwPAOL41jJx",
    "outputId": "cc79b0e6-2c5b-4e02-ce54-f6339abc07d2"
   },
   "outputs": [],
   "source": [
    "df_master_filter = df_master[df_master['volumen_despachado'] <= df_master['volumen_despachado'].quantile(0.90)].copy()\n",
    "\n",
    "plt.figure(figsize = (12, 8))\n",
    "\n",
    "for i, columna in enumerate(columnas_numericas, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    df_master_filter[columna].plot(kind = 'density', color = 'skyblue')\n",
    "    plt.xlabel('Valor')\n",
    "    plt.ylabel('Densidad')\n",
    "    plt.title(f'Gráfico de densidad de {columna}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que filtramos los datos atípicos, observamos que las variables *Cantidad* e *Importe*, tienen una distribución similar, con un ligero sesgo a la izquierda. De la misma manera aprovecharemos para eliminar todos aquellos valores que no tengan datos faltantes. De la misma manera normalizamos a nivel hora dado que esto nos permitirá hacer un mejor análisis hacia adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master_filter.reset_index(drop = True, inplace = True)\n",
    "df_master_filter.drop('Cod.Externo', axis = 1, inplace = True)\n",
    "df_master_filter.dropna(axis = 0 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master_filter['sale_timeStamp'] = df_master_filter['sale_timeStamp'].dt.floor('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmap = {' DIESEL':'diesel', ' MAGNA':'magna', ' PREMIUM':'premium'}\n",
    "df_master_filter['producto'] = df_master_filter['producto'].map(pmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master_filter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master_filter.to_pickle(f'{path}base_master.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusiones Avance 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este análisis de la base de datos de nuestro Proyecto de Modelo para Optimización de Precios en Estaciones de Autoservicio tomamos como referencia la base diaria de ventas para el periodo 2022-2024 (abril). En primer lugar, consolidamos las bases de datos asegurándonos que cumplan con el tipo de dato correspondiente a la naturaleza de cada variable para poder analizar el contenido de la base de datos. \n",
    "\n",
    "Las variables completas hacen referencia a información de la transacción de Ventas de 1 gasolinera, en la cual encontramos 3 variables numéricas, 2 variables categóricas, 2 variables que hacen referencia a la fecha de la transacción y 1 variable de identificación de la transacción. \n",
    "Encontramos valores atípicos que pueden deberse a una incorrecta digitación de la información (por ejemplo, una venta fue de 6 millones de galones) por lo cual decidimos filtrar el 10% de los valores más altos de la variable Cantidad para poder realizar el análisis exploratorio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avance 2.Ingeniería de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "Mayo 2024\n",
    "\n",
    "**Objetivos**\n",
    "    \n",
    "2.3 Crear nuevas características para mejorar el rendimiento de los modelos.\n",
    "\n",
    "2.4 Mitigar el riesgo de características sesgadas y acelerar la convergencia de algunos algoritmos.\n",
    "\n",
    "**Instrucciones**\n",
    "\n",
    "En esta fase, conocida como ingeniería de características (FE - Feature Engineering):\n",
    "\n",
    "Se aplicarán operaciones comunes para convertir los datos crudos del mundo real, en un conjunto de variables útiles para el aprendizaje automático. El procesamiento puede incluir:\n",
    "Generación de nuevas características\n",
    "Discretización o binning\n",
    "Codificación (ordinal, one hot,…)\n",
    "Escalamiento (normalización, estandarización, min – max,…)\n",
    "Transformación (logarítmica, exponencial, raíz cuadrada, Box – Cox, Yeo – Johnson,…)\n",
    "* Todas las decisiones y técnicas empleadas deben ser justificadas.\n",
    "\n",
    "\n",
    "Además, se utilizarán métodos de filtrado para la selección de características y técnicas de extracción de características, permitiendo reducir los requerimientos de almacenamiento, la complejidad del modelo y el tiempo de entrenamiento. Los ejemplos siguientes son ilustrativos, pero no exhaustivos, de lo que se podría aplicar:\n",
    "Umbral de varianza\n",
    "Correlación\n",
    "Chi-cuadrado\n",
    "ANOVA\n",
    "Análisis de componentes principales (PCA)\n",
    "Análisis factorial (FA)\n",
    "* Es necesario fundamentar los métodos ejecutados.\n",
    "\n",
    "Incluir conclusiones de la fase de \"Preparación de los datos\" en el contexto de la metodología CRISP-ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Análisis básico***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master_filter = pd.read_pickle(f'{path}base_master.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_info(df_interest, timeStamp): \n",
    "    df_interest['hour'] = df_interest[timeStamp].dt.hour\n",
    "    df_interest['month'] = df_interest[timeStamp].dt.month\n",
    "    df_interest['year'] = df_interest[timeStamp].dt.year\n",
    "    df_interest['dow_n'] = df_interest[timeStamp].dt.dayofweek\n",
    "    \n",
    "    dmap = {0:'Mon', 1:'Tue', 2:'Wed', 3:'Thu', 4:'Fri', 5:'Sat', 6:'Sun'}\n",
    "    \n",
    "    df_interest['dow'] = df_interest['dow_n'].map(dmap)\n",
    "    df_interest['clean_date'] = df_interest[timeStamp].dt.floor('D')\n",
    "\n",
    "    return df_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alt = df_master_filter.copy()\n",
    "df_alt = date_info(df_alt,'sale_timeStamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = 'dow', hue = 'producto', data = df_alt[df_alt['year'] == 2023], palette = 'viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = 'month', hue = 'producto', data = df_alt[df_alt['year'] == 2023], palette = 'viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8917xrcosMG"
   },
   "source": [
    "***Transformación de datos para procesamiento por fecha***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que estaremos trabajando con análisis de regresión para estimar demandas es necesario construir todas las estampas de manera que nos permita identificar patrones de demanda específicos (i.e. temporalidades). Es por esto que tenemos que generar un proceso de sumarización, además de asegurar que tenemos datos para la frecuencia mínima decidida (i.e. día hora) dentro los dataframes. De la misma manera se calcula el margen bruto para futuros análisis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master_summary = df_master_filter.groupby(by = ['producto','sale_timeStamp']).agg({\n",
    "    'precio_bruto' : [('precio_bruto', 'mean')],\n",
    "    'precio_neto' : [('precio_neto', 'mean')],\n",
    "    'volumen_despachado' : [('volumen_despachado', 'sum'),('transaction_num', 'count')],\n",
    "    'venta_neta' : [('venta_neta', 'sum')],\n",
    "    'venta_bruta' : [('venta_bruta', 'sum')],\n",
    "    'costo_bruto' : [('costo_bruto', 'mean')],\n",
    "    'costo_neto' : [('costo_neto', 'mean')],\n",
    "    'purchase_timeStamp' : [('purchase_timeStamp', 'min')]\n",
    "}).reset_index()\n",
    "df_master_summary.columns = [col[1] if col[1] else col[0] for col in df_master_summary.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master_summary['margen_bruto'] = df_master_summary['venta_bruta'] - df_master_summary['costo_bruto'] * df_master_summary['volumen_despachado']\n",
    "df_master_summary['margen_neto'] = df_master_summary['venta_neta'] - df_master_summary['costo_neto'] * df_master_summary['volumen_despachado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = df_master_summary['sale_timeStamp'].min()\n",
    "end_date = df_master_summary['sale_timeStamp'].max()\n",
    "\n",
    "all_dates = pd.date_range(start = start_date, end = end_date, freq = 'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_product_type(product_type):\n",
    "    product_df = df_master_summary[df_master_summary['producto'] == product_type]\n",
    "    product_df = product_df.set_index('sale_timeStamp').reindex(all_dates).rename_axis('sale_timeStamp').reset_index()\n",
    "   \n",
    "    product_df['precio_bruto'] = product_df['precio_bruto'].ffill()\n",
    "    product_df['precio_neto'] = product_df['precio_neto'].ffill()\n",
    "    \n",
    "    product_df['costo_bruto'] = product_df['costo_bruto'].ffill()\n",
    "    product_df['costo_neto'] = product_df['costo_neto'].ffill()\n",
    "\n",
    "    product_df['purchase_timeStamp'] = product_df['purchase_timeStamp'].ffill()\n",
    "    \n",
    "    product_df['venta_neta'] = product_df['venta_neta'].fillna(0)\n",
    "    product_df['venta_bruta'] = product_df['venta_bruta'].fillna(0)\n",
    "\n",
    "    product_df['volumen_despachado'] = product_df['volumen_despachado'].fillna(0)\n",
    "    product_df['transaction_num'] = product_df['transaction_num'].fillna(0)\n",
    "    product_df['margen_bruto'] = product_df['margen_bruto'].fillna(0)\n",
    "    product_df['margen_neto'] = product_df['margen_neto'].fillna(0)\n",
    "\n",
    "    product_df['precio_bruto'] = product_df['precio_bruto'].fillna(0)\n",
    "    product_df['precio_neto'] = product_df['precio_neto'].fillna(0)\n",
    "    \n",
    "    product_df['costo_bruto'] = product_df['costo_bruto'].fillna(0)\n",
    "    product_df['costo_neto'] = product_df['costo_neto'].fillna(0)\n",
    "\n",
    "    product_df['purchase_timeStamp'] = product_df['purchase_timeStamp'].fillna(start_date)\n",
    "    \n",
    "    product_df['producto'] = product_type\n",
    "    \n",
    "    return product_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_types = df_master_summary['producto'].unique()\n",
    "product_dfs = {ptype: process_product_type(ptype) for ptype in product_types}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_magna = product_dfs['magna']\n",
    "df_premium = product_dfs['premium']\n",
    "df_diesel = product_dfs['diesel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product_time = pd.concat([df_magna, df_premium, df_diesel], ignore_index = True)\n",
    "df_product_time = date_info(df_product_time,'sale_timeStamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product_time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvpN5Chjx2wo"
   },
   "source": [
    "***Análisis de Temporalidad General***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se analizó de manera general si existe un patrón de demanda en algún periodo en particular. Parte de los hallagazgos fueron que la demanda de la gasolina magna ha disminuido significativamente desde finales de 2022, mientras que la venta del combustible PREMIUM muestra una ligera tendencia creciente en el mismo periodo. Esto genera una incógnita sobre si esta caida obedece a un cámbio en la dinámica de precios o si se debe a un competidor nuevo cercano a dicha sucursal o un factor exógeno del que el equipo no tenga conocimiento. Dado que la reducción de la demanda obedece a un  proceso de rennovación, el equipo utilizara  solo los dato s de 2023 hacia adelante para reducir el sesgo al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries = df_product_time[['clean_date', 'hour', 'month', 'year', 'dow', 'volumen_despachado', 'producto','transaction_num']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "b2adE71yT1-7",
    "outputId": "6946eff8-0fa8-4dc8-8cb6-8706722f8d71"
   },
   "outputs": [],
   "source": [
    "grouped_date = df_timeseries.groupby(by = ['clean_date', 'producto']).sum().reset_index()\n",
    "grouped_dayHour = df_timeseries.drop('clean_date', axis = 1).groupby(by = ['dow', 'hour']).sum()['transaction_num'].unstack()\n",
    "grouped_dayMonth = df_timeseries.drop('clean_date', axis = 1).groupby(by = ['dow', 'month']).sum()['transaction_num'].unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14, 7))\n",
    "sns.lineplot(data = grouped_date, x = 'clean_date', y = 'volumen_despachado', hue = 'producto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general se puede observa que no existe actividad significativa los días domingos en esta sucursal lo cuál hace mucho sentido dado que es una sucursal que se encuentra al costado de una carretera que generalmente tiene la mayoría de su tráfico entre semana. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oT7NJC2d06N"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,7))\n",
    "sns.heatmap(grouped_dayHour, cmap = 'viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,7))\n",
    "sns.clustermap(grouped_dayHour, cmap = 'coolwarm', method = 'centroid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma manera se puede observar que los primeros meses del año son aquellos que existe más tráfico en la sucursal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,7))\n",
    "sns.heatmap(grouped_dayMonth, cmap = 'viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,7))\n",
    "sns.clustermap(grouped_dayMonth, cmap = 'coolwarm', method = 'centroid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Análisis de Demanda***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se incluyen los componentes de demanda que nos permitirán decidir un modelo de predicción de demanda para cada uno de los productos que existen (Magna, Premium y Diesel). Estos factores son tendencia, temporalidad y residuales y nos ayudaran a identificar si existen ciclos continuos de demanda que faciliten el entrenamiento de nuestros algorítmos. Dentro de cada uno de los productos se puede visualizar que hay un alto componente de temporalidad que se tiene que considerar a la hora de construir el modelo, lo que es un indicativo que requeriremos los diferentes niveles de las estampas de tiempo (Hora, Dia, Mes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_magna = df_magna[df_magna['sale_timeStamp'].dt.year >= 2023]\n",
    "df_premium = df_premium[df_premium['sale_timeStamp'].dt.year >= 2023]\n",
    "df_diesel = df_diesel[df_diesel['sale_timeStamp'].dt.year >= 2023]\n",
    "\n",
    "\n",
    "df_magna.set_index('sale_timeStamp', inplace = True)\n",
    "df_magna.sort_index(inplace = True)\n",
    "df_premium.set_index('sale_timeStamp', inplace = True)\n",
    "df_premium.sort_index(inplace = True)\n",
    "df_diesel.set_index('sale_timeStamp', inplace = True)\n",
    "df_diesel.sort_index(inplace = True)\n",
    "\n",
    "#daily_magna_df = df_magna['Volumen despachado'].resample('D').sum()\n",
    "#daily_premium_df = df_premium['Volumen despachado'].resample('D').sum() \n",
    "#daily_diesel_df = df_diesel['Volumen despachado'].resample('D').sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma manera utilizaremos la técnica ADF y KPSS para poder determinar el tipo de estacionalidad. Las pruebas ADF (Dickey-Fuller Aumentada) y KPSS (Kwiatkowski-Phillips-Schmidt-Shin) son herramientas estadísticas cruciales para analizar la estacionariedad de una serie temporal, elemento esencial para la modelación precisa y la inferencia en economía y finanzas. La prueba ADF evalúa si una serie presenta una raíz unitaria, indicativo de no estacionariedad, ayudando a determinar la necesidad de diferenciación de los datos para lograr estacionariedad. En contraste, la prueba KPSS verifica la hipótesis nula de estacionariedad en torno a una tendencia determinística, permitiendo detectar estacionariedad en presencia de una tendencia. La combinación de ambas pruebas proporciona un enfoque más robusto para identificar la naturaleza estacionaria de una serie, facilitando modelos predictivos más estables y confiables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](99_Datasets/adf_kpss.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adfuller_test(temps):\n",
    "    result = adfuller(temps)\n",
    "    labels = ['ADF Test Statistic', 'p-value',\n",
    "              '#Lags Used', 'Number of Observations']\n",
    "    for value, label in zip(result, labels):\n",
    "        print(label+' : '+str(value))\n",
    "\n",
    "    if result[1] <= 0.05:\n",
    "        print(\"strong evidence against the null hypothesis(Ho), reject the null hypothesis. Data IS STATIONARY\")\n",
    "    else:\n",
    "        print(\"weak evidence against null hypothesis,indicating it is non-stationary \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kpss_test(timeseries):\n",
    "    print(\"Results of KPSS Test:\")\n",
    "    kpsstest = kpss(timeseries, regression=\"c\", nlags=\"auto\")\n",
    "    kpss_output = pd.Series(\n",
    "        kpsstest[0:3], index=[\"Test Statistic\", \"p-value\", \"Lags Used\"]\n",
    "    )\n",
    "    for key, value in kpsstest[3].items():\n",
    "        kpss_output[\"Critical Value (%s)\" % key] = value\n",
    "    print(kpss_output)\n",
    "\n",
    "    if kpsstest[1] <= 0.05:\n",
    "        print(\"strong evidence against the null hypothesis(Ho), reject the null hypothesis. Data IS STATIONARY\")\n",
    "    else:\n",
    "        print(\"weak evidence against null hypothesis,indicating it is non-stationary \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Análisis de Demanda Magna***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(df_magna.index, pd.DatetimeIndex):\n",
    "    df_magna.index = pd.to_datetime(df_magna.index)\n",
    "\n",
    "print(\"Start date:\", df_magna.index[0])\n",
    "print(\"End date:\", df_magna.index[-1])\n",
    "print(\"Number of data points:\", len(df_magna))\n",
    "\n",
    "if df_magna.index[0] <= pd.Timestamp.max - pd.DateOffset(days = len(df_magna)):\n",
    "    if not df_magna.index.freq:\n",
    "        df_magna.index.freq = 'h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition_magna = sm.tsa.seasonal_decompose(df_magna['volumen_despachado'], model = 'additive')\n",
    "\n",
    "trend_magna = decomposition_magna.trend\n",
    "seasonal_magna = decomposition_magna.seasonal\n",
    "residual_magna = decomposition_magna.resid\n",
    "\n",
    "fig = decomposition_magna.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x = df_magna['volumen_despachado'].index, y = seasonal_magna, labels = {'y': 'Seasonality', 'x': 'Date'}, title = 'Seasonality Component')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistical Summary of Trend Component:\")\n",
    "print(trend_magna.describe())\n",
    "\n",
    "print(\"\\nStatistical Summary of Seasonal Component:\")\n",
    "print(seasonal_magna.describe())\n",
    "\n",
    "print(\"\\nStatistical Summary of Residual Component:\")\n",
    "print(residual_magna.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = df_magna['volumen_despachado']\n",
    "print(\"Correlation with Original Series:\")\n",
    "print(\"Trend Correlation:\", original.corr(trend_magna))\n",
    "print(\"Seasonal Correlation:\", original.corr(seasonal_magna))\n",
    "print(\"Residual Correlation:\", original.corr(residual_magna))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies_magna, spectrum_magna = periodogram(seasonal_magna.dropna(), scaling = 'spectrum')\n",
    "plt.figure()\n",
    "plt.plot(frequencies_magna, spectrum_magna)\n",
    "plt.title('Power Spectrum of the Seasonal Component')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Spectral Power')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(residual_magna.dropna())\n",
    "plt.title('Autocorrelation of Residuals')\n",
    "plt.show()\n",
    "\n",
    "plot_pacf(residual_magna.dropna())\n",
    "plt.title('Partial Autocorrelation of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basado en la prueba  de AdFuller y KPSS para magna se puede determinar que la demanda es estacionaria. No obstante el equipo observa un alto nivel de correralción con el componente estacionario por lo que lo mantendrá como parte de los parámetros del modelo hacia adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test de AdFuller')\n",
    "adfuller_test(df_magna['volumen_despachado'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test de KPSS')\n",
    "kpss_test(df_magna['volumen_despachado'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Análisis de Demanda Premium***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(df_premium.index, pd.DatetimeIndex):\n",
    "    df_premium.index = pd.to_datetime(df_premium.index)\n",
    "\n",
    "print(\"Start date:\", df_premium.index[0])\n",
    "print(\"End date:\", df_premium.index[-1])\n",
    "print(\"Number of data points:\", len(df_premium))\n",
    "\n",
    "if df_premium.index[0] <= pd.Timestamp.max - pd.DateOffset(days = len(df_premium)):\n",
    "    if not df_premium.index.freq:\n",
    "        df_premium.index.freq = 'h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition_premium = sm.tsa.seasonal_decompose(df_premium['volumen_despachado'], model = 'additive')\n",
    "\n",
    "trend_premium = decomposition_premium.trend\n",
    "seasonal_premium = decomposition_premium.seasonal\n",
    "residual_premium = decomposition_premium.resid\n",
    "\n",
    "fig = decomposition_premium.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x = df_premium['volumen_despachado'].index, y = seasonal_premium, labels = {'y': 'Seasonality', 'x': 'Date'}, title = 'Seasonality Component')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistical Summary of Trend Component:\")\n",
    "print(trend_premium.describe())\n",
    "\n",
    "print(\"\\nStatistical Summary of Seasonal Component:\")\n",
    "print(seasonal_premium.describe())\n",
    "\n",
    "print(\"\\nStatistical Summary of Residual Component:\")\n",
    "print(residual_premium.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = df_premium['volumen_despachado']\n",
    "print(\"Correlation with Original Series:\")\n",
    "print(\"Trend Correlation:\", original.corr(trend_premium))\n",
    "print(\"Seasonal Correlation:\", original.corr(seasonal_premium))\n",
    "print(\"Residual Correlation:\", original.corr(residual_premium))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies_premium, spectrum_premium = periodogram(seasonal_premium.dropna(), scaling = 'spectrum')\n",
    "plt.figure()\n",
    "plt.plot(frequencies_premium, spectrum_premium)\n",
    "plt.title('Power Spectrum of the Seasonal Component')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Spectral Power')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(residual_premium.dropna())\n",
    "plt.title('Autocorrelation of Residuals')\n",
    "plt.show()\n",
    "\n",
    "plot_pacf(residual_premium.dropna())\n",
    "plt.title('Partial Autocorrelation of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basado en la prueba  de AdFuller y KPSS para premium se puede determinar que la demanda es estacionaria. No obstante el equipo observa un alto nivel de correralción con el componente estacionario por lo que lo mantendrá como parte de los parámetros del modelo hacia adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test de AdFuller')\n",
    "adfuller_test(df_premium['volumen_despachado'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test de KPSS')\n",
    "kpss_test(df_premium['volumen_despachado'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Análisis de Demanda Diesel***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(df_diesel.index, pd.DatetimeIndex):\n",
    "    df_diesel.index = pd.to_datetime(df_diesel.index)\n",
    "\n",
    "print(\"Start date:\", df_diesel.index[0])\n",
    "print(\"End date:\", df_diesel.index[-1])\n",
    "print(\"Number of data points:\", len(df_diesel))\n",
    "\n",
    "if df_diesel.index[0] <= pd.Timestamp.max - pd.DateOffset(days = len(df_diesel)):\n",
    "    if not df_diesel.index.freq:\n",
    "        df_diesel.index.freq = 'h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition_diesel = sm.tsa.seasonal_decompose(df_diesel['volumen_despachado'], model = 'additive')\n",
    "\n",
    "trend_diesel = decomposition_diesel.trend\n",
    "seasonal_diesel = decomposition_diesel.seasonal\n",
    "residual_diesel = decomposition_diesel.resid\n",
    "\n",
    "fig = decomposition_diesel.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x = df_diesel['volumen_despachado'].index, y = seasonal_diesel, labels = {'y': 'Seasonality', 'x': 'Date'}, title = 'Seasonality Component')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistical Summary of Trend Component:\")\n",
    "print(trend_diesel.describe())\n",
    "\n",
    "print(\"\\nStatistical Summary of Seasonal Component:\")\n",
    "print(seasonal_diesel.describe())\n",
    "\n",
    "print(\"\\nStatistical Summary of Residual Component:\")\n",
    "print(residual_diesel.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = df_diesel['volumen_despachado']\n",
    "print(\"Correlation with Original Series:\")\n",
    "print(\"Trend Correlation:\", original.corr(trend_diesel))\n",
    "print(\"Seasonal Correlation:\", original.corr(seasonal_diesel))\n",
    "print(\"Residual Correlation:\", original.corr(residual_diesel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies_diesel, spectrum_diesel = periodogram(seasonal_diesel.dropna(), scaling = 'spectrum')\n",
    "plt.figure()\n",
    "plt.plot(frequencies_diesel, spectrum_diesel)\n",
    "plt.title('Power Spectrum of the Seasonal Component')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Spectral Power')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(residual_diesel.dropna())\n",
    "plt.title('Autocorrelation of Residuals')\n",
    "plt.show()\n",
    "\n",
    "plot_pacf(residual_diesel.dropna())\n",
    "plt.title('Partial Autocorrelation of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basado en la prueba  de AdFuller y KPSS para diesel se puede determinar que la demanda es estacionaria. No obstante el equipo observa un alto nivel de correralción con el componente estacionario por lo que lo mantendrá como parte de los parámetros del modelo hacia adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test de AdFuller')\n",
    "adfuller_test(df_premium['volumen_despachado'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test de KPSS')\n",
    "kpss_test(df_diesel['volumen_despachado'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_magna.to_pickle(f'{path}base_magna.pkl')\n",
    "df_premium.to_pickle(f'{path}base_premium.pkl')\n",
    "df_diesel.to_pickle(f'{path}base_diesel.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identificación de Factores Exógenos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un factor importante para determinar el precio de venta de gasolina es el precio del petróleo del cual se derivó la gasolina que se venderá. Para esto tomamos la base del EIA (US Energy Information Administration) del precio de petróleo BRENT (referencia para México). En esta sección analizaremos la relación del precio del petróleo con los precios de compra (es decir los precios a los cuales las gasolineras adquirieron el producto). Debido a que el precio internacional del petróleo no tiene un efecto inmediato sobre el precio de los proveedores, se debe realizar un análisis de correlación con rezago, para poder determinar el impacto del precio del petróleo en los precios de distribución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_master = pd.read_pickle(f'{path}base_master.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_petroleo = pd.read_csv(f'{path}Europe_Brent_Spot_Price_FOB.csv', header = None)\n",
    "db_petroleo.columns = ['fecha', 'precio_brent']\n",
    "db_petroleo['market_date'] = pd.to_datetime(db_petroleo['fecha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_days = pd.date_range(start = db_petroleo['market_date'].min(), end = db_petroleo['market_date'].max(), freq = 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_petroleo = db_petroleo.set_index('market_date').reindex(all_days).ffill()\n",
    "db_petroleo = db_petroleo.reset_index()\n",
    "db_petroleo = db_petroleo.drop(index = 0).reset_index(drop = True)\n",
    "db_petroleo = db_petroleo.rename(columns = {'index': 'market_date'})\n",
    "ppetroleo = db_petroleo[db_petroleo['market_date'] > '2021-12-30']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Básicamente podemos observar que existe una correlación visual entre el precio del petróleo Brent y el precio neto de compra al cual la gasolinera adquiere los derivados de petróleo. Con esta información visual, haremos un análisis de correlación con rezago para determinar el rezago óptimo del precio del petróleo con el costo de adquisición por tipo de combustible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_combustible = base_master.loc[base_master['producto'] == 'magna']\n",
    "\n",
    "precio_promedio_diario = base_combustible.groupby('purchase_date')['costo_neto'].mean().reset_index()\n",
    "\n",
    "precio_promedio_diario.columns = ['purchase_date', 'costo_neto']\n",
    "\n",
    "merged_db_price_cost = pd.merge(precio_promedio_diario, ppetroleo, left_on = 'purchase_date', right_on = 'market_date', how = 'inner')\n",
    "\n",
    "data_p_c = merged_db_price_cost[['purchase_date', 'costo_neto', 'market_date', 'precio_brent']]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Fecha')\n",
    "ax1.set_ylabel('Precio Brent', color = color)\n",
    "ax1.plot(data_p_c['market_date'], data_p_c['precio_brent'], color = color, label = 'Precio Brent')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('Costo Neto', color = color)\n",
    "ax2.plot(data_p_c['market_date'], data_p_c['costo_neto'], color=color, label='Costo Neto')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Título y leyenda\n",
    "plt.title('Costo Neto Promedio Diario MAGNA vs Precio diario del Petróleo Brent')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combustibles = ['magna', 'premium', 'diesel']\n",
    "\n",
    "for combustible in combustibles:\n",
    "    base_combustible = base_master.loc[base_master['producto'] == combustible]\n",
    "\n",
    "    precio_promedio_diario = base_combustible.groupby('purchase_date')['costo_neto'].mean().reset_index()\n",
    "    precio_promedio_diario.columns = ['purchase_date', 'costo_neto']\n",
    "    merged_data = pd.merge(precio_promedio_diario, ppetroleo, left_on = 'purchase_date', right_on = 'market_date', how = 'inner')\n",
    "    \n",
    "    data = merged_data[['purchase_date', 'costo_neto', 'market_date', 'precio_brent']]\n",
    "\n",
    "    lag_acf = plot_acf(data['precio_brent'], lags = 45)\n",
    "  \n",
    "    plt.xlabel('Rezago (días)')\n",
    "    plt.ylabel('Autocorrelación')\n",
    "    plt.title(f'Autocorrelación del precio del petróleo Brent {combustible}')\n",
    "    plt.show()\n",
    "\n",
    "  # Calcular la correlación entre el precio del petróleo Brent y el costo neto con diferentes rezagos\n",
    "    correlation_results = {}\n",
    "    for lag in range(1, 46):\n",
    "        data['precio_brent_lagged'] = data['precio_brent'].shift(lag)\n",
    "        correlation = data[['precio_brent_lagged', 'costo_neto']].corr().iloc[0, 1]\n",
    "        correlation_results[lag] = correlation\n",
    "\n",
    "\n",
    "    plt.plot(list(correlation_results.keys()), list(correlation_results.values()), marker = 'o')\n",
    "    plt.xlabel('Rezago (días)')\n",
    "    plt.ylabel('Correlación')\n",
    "    plt.title(f'Correlación entre el precio del petróleo Brent y el precio neto - {combustible}')\n",
    "    plt.show()\n",
    "\n",
    "    optimal_lag = max(correlation_results, key = lambda key: abs(correlation_results[key]))\n",
    "\n",
    "    print(\"Rezago óptimo:\", optimal_lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras el análisis de correlación con rezago pudimos determinar que existen diferentes rezagos óptimos para cada tipo de combustible. Para MAGNA, el rezago óptimo es de 2 días, es decir, el precio del petróleo con un rezago de 2 días tiene mayor correlación con el costo de adquisición. Para PREMIUM, el rezago óptimo es de 17 días y para DIESEL, el rezago óptimo es de 8 días. Esta diferencia en rezagos puede deberse a la naturaleza del tipo de combustible.  Debido a que encontramos esta correlación, debemos agregar la variable del precio del petróleo a nuestra base de datos. Para esto agregaremos el precio del petróleo con su rezago óptimo por cada tipo de combustible para cada data frame de combustible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_magna = pd.read_pickle(f'{path}base_magna.pkl')\n",
    "df_premium = pd.read_pickle(f'{path}base_premium.pkl')\n",
    "df_diesel = pd.read_pickle(f'{path}base_diesel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_magna['purchase_timeStamp'] = df_magna['purchase_timeStamp'].dt.floor('D')\n",
    "df_premium['purchase_timeStamp'] = df_premium['purchase_timeStamp'].dt.floor('D')\n",
    "df_diesel['purchase_timeStamp'] = df_diesel['purchase_timeStamp'].dt.floor('D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rezago_magna = 2\n",
    "rezago_premium = 17\n",
    "rezago_diesel = 8\n",
    "\n",
    "ppetroleo_magna = db_petroleo.copy()\n",
    "ppetroleo_magna['market_date'] = ppetroleo_magna['market_date'] + pd.DateOffset(days = rezago_magna)\n",
    "df_magna = pd.merge(df_magna.reset_index(), ppetroleo_magna.drop(['fecha'], axis = 1), left_on = 'purchase_timeStamp', right_on = 'market_date', how = 'inner')\n",
    "df_magna.drop('market_date', axis = 1, inplace = True)\n",
    "df_magna = date_info(df_magna,'sale_timeStamp')\n",
    "\n",
    "ppetroleo_premium = db_petroleo.copy()\n",
    "ppetroleo_premium['market_date'] = ppetroleo_premium['market_date'] + pd.DateOffset(days = rezago_premium)\n",
    "df_premium = pd.merge(df_premium.reset_index(), ppetroleo_premium.drop(['fecha'], axis = 1), left_on = 'purchase_timeStamp', right_on = 'market_date', how = 'inner')\n",
    "df_premium.drop('market_date', axis = 1, inplace = True)\n",
    "df_premium = date_info(df_premium,'sale_timeStamp')\n",
    "\n",
    "ppetroleo_diesel = db_petroleo.copy()\n",
    "ppetroleo_diesel['market_date'] = ppetroleo_diesel['market_date'] + pd.DateOffset(days = rezago_diesel)\n",
    "df_diesel = pd.merge(df_diesel.reset_index(), ppetroleo_diesel.drop(['fecha'], axis = 1), left_on = 'purchase_timeStamp', right_on = 'market_date', how = 'inner')\n",
    "df_diesel.drop('market_date', axis = 1, inplace = True)\n",
    "df_diesel = date_info(df_diesel,'sale_timeStamp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_magna.to_pickle(f'{path}base_magna_final.pkl')\n",
    "df_premium.to_pickle(f'{path}base_premium_final.pkl')\n",
    "df_diesel.to_pickle(f'{path}base_diesel_final.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusiones Avance 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nuestro modelo de optimización de precios la preparación de datos es el paso más importante previo al modelamiento debido a que logramos comprender la base de datos disponible por parte de las gasolineras, al igual que el contexto económico del negocio y las variables que afectan las ventas de combustible. \n",
    "\n",
    "En la comprensión de la base de datos, identificamos variables de oferta y demanda de las gasolineras al igual que obtuvimos nuestra variable dependiente para nuestro modelo que es el Margen de Ganancias. De igual manera, obtuvimos información sobre el comportamiento temporal de nuestras variables para extraer insights sobre el funcionamiento del negocio y su estado actual (como mencionamos anteriormente, encontramos una reducción en el volumen de combustible vendido a lo largo del periodo 2022- 2024). También ligamos el comportamiento de los costos de combustibles al precio internacional del petróleo, de tal manera que obtuvimos una variable externa que nos otorga bastante información que puede ser útil en el modelamiento de optimización de precios.\n",
    "\n",
    "También comprendimos que es necesaria la división de nuestra base de datos en tipo de Combustible ofertado. Esto debido a que la estacionalidad y tendencia de ventas varía de acuerdo con el tipo de combustible, al igual que el precio del petróleo influye de diferente manera a cada tipo de combustible. Por lo cual, en lugar de crear variables dummies para cada tipo de combustible, vamos a crear un modelo especifico para cada tipo de combustible disponible en nuestra base de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avance 3.Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "Mayo 2024\n",
    "\n",
    "**Objetivos**\n",
    "    \n",
    "3.1 Establecer las medidas de calidad del modelo de aprendizaje automático.\n",
    "\n",
    "3.2 Proporcionar un marco de referencia para evaluar y mejorar modelos más avanzado\n",
    ".\n",
    "\n",
    "**Instruccion***\n",
    "\n",
    "Este avance consiste en construir un modelo de referencia que permita evaluar la viabilidad del problema. Si el baseline tiene un rendimiento similar al azar, podría indicar que el problema es intrínsecamente difícil o que los datos no contienen suficiente información para predecir el objetivo. De lo contrario, el baseline podría como una solución mínima aceptable cuando se trabaja en escenarios donde incluso un modelo simple puede proporcionar valor.\n",
    "\n",
    "Un baseline facilita también la gestión de expectativas, tanto dentro del equipo como con los stakeholders, pues proporciona una comprensión inicial de lo que se puede lograr con métodos simples antes de invertir tiempo y recursos en enfoques más complejos.\n",
    "\n",
    "Las siguientes son algunas de las preguntas que deberán abordar durante esta fase:\n",
    "\n",
    "¿Qué algoritmo se puede utilizar como baseline para predecir las variables objetivo? \n",
    "¿Se puede determinar la importancia de las características para el modelo generado? Recuerden que incluir características irrelevantes puede afectar negativamente el rendimiento del modelo y aumentar la complejidad sin beneficios sustanciales.\n",
    "¿El modelo está sub/sobreajustando los datos de entrenamiento?\n",
    "¿Cuál es la métrica adecuada para este problema de negocio? \n",
    "¿Cuál debería ser el desempeño mínimo obtener?\n",
    " o real, en un conjunto de variables útiles para el aprendizaje automático. El procesamiento puede incluir:\n",
    "Generación de nuevas características\n",
    "Discretización o binning\n",
    "Codificación (ordinal, one hot,…)\n",
    "Escalamiento (normalización, estandarización, min – max,…)\n",
    "Transformación (logarítmica, exponencial, raíz cuadrada, Box – Cox, Yeo – Johnson,…)\n",
    "* Todas las decisiones y técnicas empleadas deben ser justificadas.\n",
    "\n",
    "\n",
    "Además, se utilizarán métodos de filtrado para la selección de características y técnicas de extracción de características, permitiendo reducir los requerimientos de almacenamiento, la complejidad del modelo y el tiempo de entrenamiento. Los ejemplos siguientes son ilustrativos, pero no exhaustivos, de lo que se podría aplicar:\n",
    "Umbral de varianza\n",
    "Correlación\n",
    "Chi-cuadrado\n",
    "ANOVA\n",
    "Análisis de componentes principales (PCA)\n",
    "Análisis factorial (FA)\n",
    "* Es necesario fundamentar los métodos ejecutados.\n",
    "\n",
    "Incluir conclusiones de la fase de \"Preparación de los datos\" en el contexto de la metodología CRISP-ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nuestro modelo de optimización de precios de gasolina, reiteramos que vamos a trabajar sobre 3 tipos de combustible por separado (MAGNA, PREMIUM, DIESEL), de tal manera que obtendremos 3 modelos diferentes de optimización de precios. Para nuestro modelo baseline vamos a elegir un modelo de Random Forest para la demanda (volumen_despachado) y un modelo genético para la determinación del precio. Elegimos este tipo de algortimo porque nos facilita capturar las relaciones de mayor complejidad entre variable dependiente y variables independientes. De igual manera, el Random Forest ayuda a la reducción de *overfitting*. También porque nos permite identificar las variables importantes en nuestra predicción, de tal manera que podamos  eliminar aquellas que no afectan significativamente a la variable objetivo y puedan causar ruido en la predicción si son incluidas. De igual manera, utilizamos un modelo genético debido a su capacidad de capturar relaciones complejas entre las variables, la capacidad de incorporar restricciones a las diferentes variables del negocio, la penalización a la variable objetivo, que en este caso evita que el precio óptimo se encuentre en un rango fuera de lo normal (es decir, evita que el precio óptimo sea absurdamente alto porque a mayor precio mayor rentabilidad, y lo ubica dentro de un rango razonable dentro de las datos observados). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_magna = pd.read_pickle(f'{path}base_magna_final.pkl')\n",
    "df_premium = pd.read_pickle(f'{path}base_premium_final.pkl')\n",
    "df_diesel = pd.read_pickle(f'{path}base_diesel_final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_unix_time(date):\n",
    "    return int((date - datetime(1970, 1, 1)).total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de desarrollar el modelo realizamos una codificación de las variables temporales para capturar el ciclo temporal en los datos. Esto se realiza con el seno y coseno de las variables temporales de hora, día y mes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df_interest):\n",
    "    df_interest['hour_sin'] = np.sin(2 * np.pi * df_interest['hour']/23.0)\n",
    "    df_interest['hour_cos'] = np.cos(2 * np.pi * df_interest['hour']/23.0)\n",
    "    df_interest['day_sin'] = np.sin(2 * np.pi * df_interest['dow_n']/6.0)\n",
    "    df_interest['day_cos'] = np.cos(2 * np.pi * df_interest['dow_n']/6.0)\n",
    "    df_interest['month_sin'] = np.sin(2 * np.pi * df_interest['month']/11.0)\n",
    "    df_interest['month_cos'] = np.cos(2 * np.pi * df_interest['month']/11.0)\n",
    "    df_interest['unix_time'] = df_interest['sale_timeStamp'].apply(convert_to_unix_time)\n",
    "    df_interest.drop(['sale_timeStamp'], axis = 1, inplace = True)\n",
    "    return df_interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra variable que estamos agregando nuestro modelo es la elasticidad de la demanda. Esta variable también es relevante porque nos ayudará a determinar como cambia la demanda cuando el precio se modifica. La elasticidad es importante porque nos da información sobre el producto, por ejemplo, si una pequeña variación del precio genera un cambio significativo en la demanda entonces el producto se considera elástico); o pese a un cambio importante en el precio, la cantidad demandada se mantiene igual, el producto se considera inelástico. Para obtener la elasticidad de la demanda estamos realizando una regresión linear con los logaritmos de nuestros datos de precio_neto y volumen_despachado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_features = ['volumen_despachado', 'precio_neto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pricing_elasticity_prep(df_interest,features):\n",
    "    new_df = df_interest[features].copy()\n",
    "    type_dict = {}\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        type_dict[i] = df_interest[feature].dtype\n",
    "    \n",
    "    new_df.replace(0, pd.NA, inplace = True)\n",
    "    new_df.dropna(inplace = True)\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        new_df[feature] = new_df[feature].astype(type_dict[i])\n",
    "\n",
    "    new_df['log_price'] = np.log(new_df['precio_neto'])\n",
    "    new_df['log_demand'] = np.log(new_df['volumen_despachado'])\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_magna_alt = pricing_elasticity_prep(df_magna, pe_features)\n",
    "df_premium_alt = pricing_elasticity_prep(df_premium, pe_features)\n",
    "df_diesel_alt = pricing_elasticity_prep(df_diesel, pe_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_elasticity_magna = LinearRegression()\n",
    "model_lr_elasticity_magna.fit(df_magna_alt[['log_price']], df_magna_alt[['log_demand']])\n",
    "price_elasticity_magna = model_lr_elasticity_magna.coef_[0]\n",
    "\n",
    "model_lr_elasticity_premium = LinearRegression()\n",
    "model_lr_elasticity_premium.fit(df_premium_alt[['log_price']], df_premium_alt[['log_demand']])\n",
    "price_elasticity_premium = model_lr_elasticity_premium.coef_[0]\n",
    "\n",
    "model_lr_elasticity_diesel = LinearRegression()\n",
    "model_lr_elasticity_diesel.fit(df_diesel_alt[['log_price']], df_diesel_alt[['log_demand']])\n",
    "price_elasticity_diesel = model_lr_elasticity_diesel.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Elasticidad Magna: {price_elasticity_magna[0]:.2f}')\n",
    "print(f'Elasticidad Premium: {price_elasticity_premium[0]:.2f}')\n",
    "print(f'Elasticidad Diesel: {price_elasticity_diesel[0]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generación de Modelo Base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_data_prep(df_interest, features, target):\n",
    "    alt_df = feature_engineering(df_interest[features].copy())\n",
    "    target_df = df_interest[target].copy()\n",
    "\n",
    "    new_df = pd.concat([alt_df, target_df], axis = 1)\n",
    "    \n",
    "    train_percentage = 0.8\n",
    "    slice_point = int(new_df.shape[0] * train_percentage)\n",
    "    \n",
    "    train_df = new_df.iloc[:slice_point]\n",
    "    test_df = new_df.iloc[slice_point:]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    X_train = scaler.fit_transform(train_df[alt_df.columns])\n",
    "    X_test = scaler.transform(test_df[alt_df.columns])\n",
    "\n",
    "    y_train = train_df[target].values.ravel()\n",
    "    y_test = test_df[target].values.ravel()\n",
    "\n",
    "    return alt_df, scaler, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_grid_search(model, param_grid, X_train, X_test, y_train, y_test):\n",
    "    grid_search = GridSearchCV(estimator = model, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "    return  best_rf, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metrics(y_test, y_pred):\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred, force_finite = False)\n",
    "    print(f'Demand Prediction Model MSE: {mse}, R²: {r2})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_rf (df_interest, features, target):\n",
    "    \n",
    "    alt_df, scaler, X_train, X_test, y_train, y_test = model_data_prep(df_interest, features, target)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "\n",
    "    model = RandomForestRegressor(random_state = 42)\n",
    "    best_rf, y_pred = model_grid_search(model, param_grid, X_train, X_test, y_train, y_test)\n",
    "    performance_metrics(y_test, y_pred)\n",
    "    \n",
    "    return best_rf, X_train, scaler, df_interest['precio_neto'].mean(), alt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optmización de Modelo: Modelo Genético**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que obtenemos el RandomForest del volumen despachado, procedemos a realizar un algoritmo genético para la optimización de precio. Dentro del modelo genético estamos generando individuos, población, cruce, mutación y selección. Una vez que definimos estos parametros procedemos a calcular el margun bruto utilizando nuestra base de datos de entrenamiento, nuestro modelo RandomForest, la elasticidad del precio (que calculamos con regresión lineal) y el precio actual. Por último, se incluye una penalización calculada en función a la diferencia del precio_pred con el precio actual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_toolbox(model, train_data, scaler, current_price, lower_bound_ratio, upper_bound_ratio, elasticity, feature_names):\n",
    "    \n",
    "    creator.create('FitnessMax', base.Fitness, weights = (1.0,))\n",
    "    creator.create('Individual', list, fitness=creator.FitnessMax)\n",
    "\n",
    "    lower_bound = current_price * lower_bound_ratio\n",
    "    upper_bound = current_price * upper_bound_ratio\n",
    "    \n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register('attr_float', random.uniform, lower_bound, upper_bound)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.attr_float, n=1)\n",
    "    toolbox.register('population', tools.initRepeat, list, toolbox.individual)\n",
    "    toolbox.register('evaluate', eval_gross_margin, train_data = train_data, model = model, \n",
    "                     scaler = scaler, current_price = current_price, elasticity = elasticity, feature_names = feature_names, type = 'max')\n",
    "    toolbox.register('mate', tools.cxBlend, alpha = 0.5)\n",
    "    toolbox.register('mutate', tools.mutGaussian, mu = 0, sigma = 1, indpb = 0.2)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 3)\n",
    "    \n",
    "    return toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_gross_margin(individual, train_data, model, scaler, current_price, elasticity, feature_names, type = 'max', delta = 0.1):\n",
    "    try:\n",
    "        price = individual[0]\n",
    "        # Create a dataframe to simulate the scenario with the evaluated price\n",
    "        mean_values = np.mean(train_data[:, 1:], axis=0) \n",
    "        data_with_price = np.insert(mean_values, 0, price)\n",
    "        temp_features = pd.DataFrame([data_with_price], columns = feature_names)\n",
    "        \n",
    "        # Calculate the scaled features for the model\n",
    "        scaled_features = scaler.transform(temp_features)\n",
    "        predicted_demand = model.predict(scaled_features)[0]\n",
    "        \n",
    "        # Extract cost of goods sold from the temporary features, ensuring it corresponds to the simulated scenario\n",
    "        cost_of_goods_sold = temp_features['costo_neto'].iloc[0]\n",
    "        \n",
    "        gross_margin = (price - cost_of_goods_sold) * predicted_demand\n",
    "\n",
    "        penalty_factor = 100\n",
    "        price_deviation = abs(price - current_price) / current_price\n",
    "        penalty = penalty_factor * max(0, price_deviation - delta) ** (-elasticity)\n",
    "        \n",
    "        gross_margin -= penalty\n",
    "        if type == 'max':\n",
    "            return (gross_margin,)\n",
    "        if type == 'min':\n",
    "            return (-gross_margin,)\n",
    "    except Exception as e:\n",
    "        print(\"Error in evalGrossMargin:\", e)\n",
    "        return (0,)  # Return a default or zero fitness in case of error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_price_gen(df, technique, features, target, elasticity, lower_bound_ratio, upper_bound_ratio):\n",
    "    model, X_train, scaler, current_price, features_refined = technique(df, features, target)\n",
    "    \n",
    "    lower_bound = current_price * lower_bound_ratio\n",
    "    upper_bound = current_price * upper_bound_ratio\n",
    "    \n",
    "    toolbox = init_toolbox(model, X_train, scaler, current_price, lower_bound_ratio, upper_bound_ratio, elasticity, features_refined.columns)\n",
    "\n",
    "    population = toolbox.population(n = 50)\n",
    "    NGEN, CXPB, MUTPB = 40, 0.5, 0.2\n",
    "\n",
    "    for gen in range(NGEN):\n",
    "        offspring = algorithms.varAnd(population, toolbox, cxpb = CXPB, mutpb = MUTPB)\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = list(map(toolbox.evaluate, invalid_ind))\n",
    "        \n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        population[:] = toolbox.select(offspring, len(population))\n",
    "        fits = [ind.fitness.values[0] for ind in population]\n",
    "        if gen == 0 or gen == NGEN -1:\n",
    "            print(f\"Generation {gen}: Max Fit {max(fits)}, Avg Fit {np.mean(fits)}\")\n",
    "\n",
    "    top_individual = tools.selBest(population, k = 1)[0]\n",
    "    optimal_price = top_individual[0]\n",
    "    max_gross_margin = top_individual.fitness.values[0][0]\n",
    "\n",
    "    print(f'Optimal price: ${optimal_price:.2f}')\n",
    "    print(f'Maximum Gross Margin: ${max_gross_margin:.2f}')\n",
    "\n",
    "    creator.__delattr__('FitnessMax')\n",
    "    creator.__delattr__('Individual')\n",
    "    \n",
    "    return optimal_price, max_gross_margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra la construcción de la función de optimización Fitness:\n",
    "\n",
    "Empezamos con el cálculo del márgen bruto utilizando el precio evaluado\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?GM&space;=\\ (P_e&space;-&space;CoGS)\\cdot&space;\\hat{D}\" title=\"GM&space;=\\ (P_R&space;-&space;CoGS)\\hat{D}\" />\n",
    "\n",
    "\n",
    "Luego definimos la función de penalización basada en la elasticidad de la demanda, el precio corriente, el precio evaluado y un factor. Este penalty considera una función bisagra para penalizar precios por encima de un límite\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\xi&space;=\\ PF\\cdot&space; \\max(0,(\\frac{|P_e&space;-&space;P_c|}{P_c})^\\epsilon - \\delta)\" title=\"\\xi&space;=\\ PF\\cdot&space; \\max(0,(\\frac{|P_e&space;-&space;P_c|}{P_c})^\\epsilon - \\delta)\" />\n",
    "\n",
    "Función de Fitness:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?Fitness&space;= GM&space;-&space;\\xi\" title=\"\\xi&space;=\\ PF\\cdot&space; (\\frac{|P_e&space;-&space;P_c|}{P_c})^\\epsilon\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para los features de nuestro modelo estamos incluyendo precio_neto, costo_neto, precio_brent (rezagado), y variables temporales de hora, día y mes. Nuestra variable objetivo es el volumen despachado o como lo definimos la demanda de la gasolinera. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = ['precio_neto', 'costo_neto', 'precio_brent', 'hour', 'month', 'dow_n','sale_timeStamp']\n",
    "model_target = ['volumen_despachado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magna_price, magna_gross_margin = optimize_price_gen(df_magna, setup_model_rf, model_features, model_target, price_elasticity_magna, 0.7, 1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premium_price, premium_gross_margin = optimize_price_gen(df_premium, setup_model_rf, model_features, model_target, price_elasticity_premium, 0.7, 1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diesel_price, diesel_gross_margin = optimize_price_gen(df_diesel, setup_model_rf, model_features, model_target, price_elasticity_diesel, 0.7, 1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusiones Avance 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basados en los resultados obtenidos el equipo ha identificado la necesidad de invertir más recursos para mejorar la precisión del modelo de demanda ya sea a través de el uso de otros modelos o una optimización local de cada modelo (i.e. GridSearchCSV). Sin lugar a duda la adición del algoritmo genético tiene mucho potencial debido a su capacidad para explorar grandes espacios de soluciones y encontrar resultados cercanos al óptimo global. Su enfoque basado en la selección, mutación y cruce permite ajustar dinámicamente los precios en función de la demanda. Además, su flexibilidad y adaptabilidad los hace adecuados para entornos complejos y cambiantes. No obstante, de la misma manera es importante evaluar si el algorítmo genético es adecuado ya que los precios sugeridos se encuentran fuera de los límites impuestos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avance 4. Modelos alternativos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mayo 2024\n",
    "\n",
    "**Objetivos**\n",
    "\n",
    "3.3 Explorar una gama diversa de técnicas y enfoques con el fin de identificar el de mejor desempeño en el conjunto de datos en cuestión.\n",
    "\n",
    "3.4 Encontrar la configuración óptima que maximiza el rendimiento del modelo en una tarea específica.\n",
    "\n",
    "**Instrucciones**\n",
    "\n",
    "Este avance implica construir múltiples modelos (individuales, no ensambles) relevantes para resolver el problema y evaluar su desempeño. Diferentes algoritmos pueden comportarse de manera óptima en diferentes tipos de datos o tareas. La construcción de modelos alternativos permite explorar y evaluar cuál de ellos proporciona el mejor rendimiento para un problema particular.\n",
    "\n",
    "Además, los modelos se pueden ajustar para determinar si se puede mejorar su rendimiento. Diferentes configuraciones de hiperparámetros pueden afectar significativamente el rendimiento de un modelo. Construir modelos alternativos implica explorar y ajustar estos hiperparámetros para encontrar la configuración óptima.\n",
    "\n",
    "Las siguientes son acciones que deberás abordar en este avance:\n",
    "\n",
    "Construir al menos 6 modelos diferentes (individuales, no ensambles), utilizando algoritmos variados.\n",
    "Comparar el rendimiento de los modelos obtenidos.\n",
    "Seleccionar los dos modelos que proporcionen el mejor rendimiento.\n",
    "Ajustar los dos mejores modelos.\n",
    "Elegir el modelo individual final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelos alternativos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaciín desglosamos todos los modelos que el equipo probó para poder determinar la demanda de los productos\n",
    "\n",
    "**Random Forest**: Este método utiliza múltiples árboles de decisión para realizar predicciones, lo cual lo hace menos propenso al sobreajuste comparado con un solo árbol de decisión. Es muy efectivo para capturar complejidades en los datos sin necesidad de configuraciones extensivas, lo que lo hace muy útil para series temporales no lineales y complejas.\n",
    "\n",
    "**Gradient Boosting**: Consiste en construir modelos predictivos en forma de un ensamble de árboles de decisión débiles. Se adapta bien a irregularidades en los datos al optimizar una función de pérdida, siendo muy efectivo para reducir el error de predicción y mejorar la precisión en series temporales con tendencias y estacionalidades.\n",
    "\n",
    "**XGBoost**: Es una implementación optimizada de la potenciación del gradiente que es famosa por su eficiencia y rendimiento en competiciones de ciencia de datos. XGBoost es particularmente potente en términos de velocidad y rendimiento en grandes conjuntos de datos, con capacidades avanzadas para manejar datos faltantes en series temporales.\n",
    "\n",
    "**LightGBM**: Similar a XGBoost, pero más eficiente en el manejo de grandes volúmenes de datos. Utiliza técnicas de aprendizaje basadas en gradientes pero con un enfoque en la eficiencia de la memoria y la velocidad de procesamiento, lo que lo hace ideal para series temporales donde la velocidad y el uso eficiente de la memoria son críticos.\n",
    "\n",
    "**SVR (Support Vector Regression)**: Esta variante de las SVM se utiliza para la regresión, encontrando la línea (o hiperplano en dimensiones superiores) que mejor se ajusta al modelo. Es muy efectivo en series temporales con un claro margen de separación y puede ser altamente personalizable con diferentes kernels.\n",
    "\n",
    "**KNN (K-Nearest Neighbors)**: Un método no paramétrico que predice el valor de un nuevo punto basado en las 'k' observaciones más cercanas. Este método es simple pero efectivo, especialmente en series temporales cortas y no estacionarias, donde las similitudes entre patrones cercanos en tiempo pueden predecir futuras tendencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_gb (df_interest, features, target):\n",
    "    \n",
    "    alt_df, scaler, X_train, X_test, y_train, y_test = model_data_prep(df_interest, features, target)\n",
    "\n",
    "    param_grid  = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "    }\n",
    "    \n",
    "    model = GradientBoostingRegressor(random_state = 42)\n",
    "    \n",
    "    best_rf, y_pred = model_grid_search(model, param_grid, X_train, X_test, y_train, y_test)\n",
    "    performance_metrics(y_test, y_pred)\n",
    "    \n",
    "    return best_rf, X_train, scaler, df_interest['precio_neto'].mean(), alt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_xgb (df_interest, features, target):\n",
    "    \n",
    "    alt_df, scaler, X_train, X_test, y_train, y_test = model_data_prep(df_interest, features, target)\n",
    "\n",
    "    param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor(random_state = 42)\n",
    "    \n",
    "    best_rf, y_pred = model_grid_search(model, param_grid, X_train, X_test, y_train, y_test)\n",
    "    performance_metrics(y_test, y_pred)\n",
    "    \n",
    "    return best_rf, X_train, scaler, df_interest['precio_neto'].mean(), alt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_lgb (df_interest, features, target):\n",
    "    \n",
    "    alt_df, scaler, X_train, X_test, y_train, y_test = model_data_prep(df_interest, features, target)\n",
    "\n",
    "    param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "    }\n",
    "    \n",
    "    model = LGBMRegressor(random_state = 42, verbosity = -1)\n",
    "    \n",
    "    best_rf, y_pred = model_grid_search(model, param_grid, X_train, X_test, y_train, y_test)\n",
    "    performance_metrics(y_test, y_pred)\n",
    "    \n",
    "    return best_rf, X_train, scaler, df_interest['precio_neto'].mean(), alt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_svr (df_interest, features, target):\n",
    "    \n",
    "    alt_df, scaler, X_train, X_test, y_train, y_test = model_data_prep(df_interest, features, target)\n",
    "\n",
    "    param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'epsilon': [0.01, 0.1, 1],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "    }\n",
    "\n",
    "    model = SVR()\n",
    "    \n",
    "    best_rf, y_pred = model_grid_search(model, param_grid, X_train, X_test, y_train, y_test)\n",
    "    performance_metrics(y_test, y_pred)\n",
    "    \n",
    "    return best_rf, X_train, scaler, df_interest['precio_neto'].mean(), alt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_knn (df_interest, features, target):\n",
    "    \n",
    "    alt_df, scaler, X_train, X_test, y_train, y_test = model_data_prep(df_interest, features, target)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 10],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree']\n",
    "    }\n",
    "\n",
    "    model = KNeighborsRegressor()\n",
    "    \n",
    "    best_rf, y_pred = model_grid_search(model, param_grid, X_train, X_test, y_train, y_test)\n",
    "    performance_metrics(y_test, y_pred)\n",
    "    \n",
    "    return best_rf, X_train, scaler, df_interest['precio_neto'].mean(), alt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optmización de Modelo Alternativo: Evolución Diferenciada**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos el método de evolución diferenciada como método adicional comparativo ya que este es un método robusto y eficaz para optimizar funciones de pérdida en problemas de aprendizaje automático. Esta técnica se destaca por su simplicidad y su capacidad para manejar espacios de búsqueda grandes y complejos. Además, es menos probable que se quede atrapada en mínimos locales en comparación con otros métodos de optimización, lo que resulta en una mejora en la calidad de las soluciones encontradas. Su adaptabilidad a diferentes tipos de problemas y la facilidad de implementación la hacen ideal para una amplia variedad de aplicaciones en el aprendizaje automático."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_price_diff(df, technique, features, target, elasticity, lower_bound_ratio, upper_bound_ratio):\n",
    "    model, X_train, scaler, current_price, features_refined = technique(df, features, target)\n",
    "\n",
    "    lower_bound = current_price * lower_bound_ratio\n",
    "    upper_bound = current_price * upper_bound_ratio\n",
    "    \n",
    "    bounds = [(lower_bound, upper_bound)]\n",
    "\n",
    "    result = differential_evolution(eval_gross_margin, bounds,\n",
    "                                    args = (X_train, model, scaler, current_price, elasticity, features_refined.columns, 'max'),\n",
    "                                    strategy = 'best1bin', maxiter = 1000, popsize = 50, tol = 0.01, mutation = (0.5, 1), recombination = 0.7, disp = True)\n",
    "\n",
    "    optimal_price = result.x[0]\n",
    "    max_gross_margin = result.fun\n",
    "\n",
    "    print(f'Optimal price: ${optimal_price:.2f}')\n",
    "    print(f'Maximum Gross Margin: ${max_gross_margin:.2f}')\n",
    "\n",
    "    return optimal_price, max_gross_margin\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pruebas generales**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos una corrida con todas las diferentes combinaciones para determinar cuál es el mejor modelo que maximiza la evaluación de la demanda, así como cuál esl mejor proceso de optimización se adecua mejor a nuestros requerimientos. Como se puede observar más abajo los mejores modelos para cada combustible basado en R² son los siguientes:\n",
    "\n",
    "**Magna** - Light GBM (R² = 80.2%)\n",
    "\n",
    "**Premium** - Random Forest (R² = 49.2%)\n",
    "\n",
    "**Diesel** - XGBoost (R² = 33.4%)\n",
    "\n",
    "\n",
    "De la misma manera encontramos que la evolución diferencial es más práctica para determinar una solución local que converge dentro de los límites impuestos para los precios por lo que hemos decido migrar a ese método de optimización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "        'Magna': df_magna,\n",
    "        'Premium': df_premium,\n",
    "        'Diesel': df_diesel\n",
    "}\n",
    "\n",
    "optimizers = {\n",
    "        'Genetic Algorithm': optimize_price_gen,\n",
    "        'Differential evolution': optimize_price_diff\n",
    "}\n",
    "\n",
    "models = {\n",
    "        'RandomForest': setup_model_rf,\n",
    "        'GradientBoosting': setup_model_gb,\n",
    "        'XGBoost': setup_model_xgb,\n",
    "        'LightGBM': setup_model_lgb,\n",
    "        'SVR': setup_model_svr,\n",
    "        'KNN': setup_model_knn\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for dataframe_name, dataframe in dataframes.items():\n",
    "    print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    print(f'Combustible: {dataframe_name}')\n",
    "    \n",
    "    for optimizer_name, optimizer in optimizers.items():\n",
    "        print('*****************************************************************************')\n",
    "        print(f'Combustible: {dataframe_name} | Optimizador Evaluado: {optimizer_name}')\n",
    "    \n",
    "        for model_name, model in models.items():\n",
    "            print('=============================================================================')\n",
    "            print(f'Combustible: {dataframe_name} | Optimizador Evaluado: {optimizer_name} | Modelo Evaluado: {model_name}')\n",
    "            price, gross_margin = optimizer(dataframe, model, model_features, model_target, price_elasticity_magna, 0.7, 1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe una diferencia en el desempeño del Algoritmo Genético y Evolución Diferencial. En general, el Algoritmo Genético demostró ser más eficaz en la mayoría de los casos, especialmente para los combustibles Magna y Diesel.  En algunos casos, existen inconsistencias que deben de ser separadas y otras atendidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusiones Avance 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los objetivos de este avance fueron explorar una gama diversa de técnicas y enfoques para identificar el mejor desempeño en el conjunto de datos en cuestión y encontrar la configuración óptima que maximice el rendimiento del modelo en la tarea específica de predicción de demanda. Probamos seis modelos diferentes (Random Forest, Gradient Boosting, XGBoost, LightGBM, Support Vector Regression (SVR), K-Nearest Neighbors (KNN)) y dos métodos de optimización (Algoritmo Genético y Evolución Diferencial) para cada tipo de combustible (Magna, Premium, Diesel).\n",
    "\n",
    "Para Magna, el mejor modelo fue LightGBM con un R² de 80.2%, y el mejor método de optimización fue Evolución Diferencial. Para Premium, el mejor modelo fue Random Forest, y el mejor método de optimización fue Algoritmo Genético, alcanzando un máximo margen bruto de utilizando SVR con Algoritmo Genético. Para Diesel, el mejor modelo fue XGBoost, y el mejor método de optimización fue Evolución Diferencial,  utilizando XGBoost con Algoritmo Genético no obstante .\n",
    "\n",
    "Observamos que LightGBM y XGBoost demostraron ser consistentemente fuertes en la predicción de demanda de combustibles, especialmente en el caso de Magna y Diesel respectivamente. Evolución Diferencial resultó ser el método de optimización más eficiente en general, proporcionando soluciones locales óptimas dentro de los límites impuestos. Algoritmo Genético mostró mejores resultados en algunos casos específicos, como con SVR para Premium.\n",
    "\n",
    "En conclusión, la exploración y comparación de múltiples modelos y técnicas de optimización han permitido identificar configuraciones óptimas para cada tipo de combustible. Esta aproximación mejora la precisión de la predicción de demanda y predice el precio optimo, sin embargo como se puede ver, tenemos que utilizar otros descriptores a los cuales no tenemos acceso, para una mayor predicción."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
