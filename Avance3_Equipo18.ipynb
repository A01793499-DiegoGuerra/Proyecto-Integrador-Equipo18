{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TC5035.10 Proyecto Integrador\n",
    " \n",
    "Dra. Grettel Barceló Alonso\n",
    "Dr. Luis Eduardo Falcón Morales\n",
    "\n",
    "Liga Github: https://github.com/A01793499-DiegoGuerra/Proyecto-Integrador-Equipo18/tree/main\n",
    "\n",
    "#### Equipo 18 : “Modelos para la Optimización de Precios en Estaciones de Autoservicio”\n",
    "\n",
    "* Diego Fernando Guerra Burgos\tA01793499\n",
    "* Esteban Sánchez Retamoza\t\tA01740631\n",
    "* Hansel Zapiain Rodríguez\t\tA00469031uipo docente. to\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avance 1. Análisis exploratorio de datos**\n",
    "   \n",
    "Abril 2024\n",
    "\n",
    "**Objetivos**\n",
    "\n",
    "2.1 Elegir las características más relevantes para reducir la dimensionalidad y aumentar la capacidad de generalización del modelo.\n",
    "\n",
    "2.2 Abordar y corregir los problemas identificados en los datos.\n",
    "\n",
    "**Instrucciones**\n",
    "\n",
    "Este primer avance consiste en realizar un análisis exploratorio de datos (EDA - Exploratory Data Analysis), es decir, describir los datos utilizando técnicas estadísticas y de visualización (análisis univariante y bi/multivariante) para hacer enfoque en sus aspectos más relevantes, así como aplicar y justificar operaciones de preprocesamiento, relacionadas con el manejo de valores faltantes, atípicos y alta cardinalidad. Es importante que incluyan sus conclusiones del EDA, identificando tendencias o relaciones importantes.\n",
    "\n",
    "Las siguientes son algunas de las preguntas comunes que podrán abordar a través del EDA:\n",
    "\n",
    "¿Hay valores faltantes en el conjunto de datos? ¿Se pueden identificar patrones de ausencia? \n",
    "¿Cuáles son las estadísticas resumidas del conjunto de datos?\n",
    "¿Hay valores atípicos en el conjunto de datos?\n",
    "¿Cuál es la cardinalidad de las variables categóricas?\n",
    "¿Existen distribuciones sesgadas en el conjunto de datos? ¿Necesitamos aplicar alguna transformación no lineal?\n",
    "¿Se identifican tendencias temporales? (En caso de que el conjunto incluya una dimensión de tiempo).\n",
    "¿Hay correlación entre las variables dependientes e independientes?\n",
    "¿Cómo se distribuyen los datos en función de diferentes categorías?\n",
    "¿Existen patrones o agrupaciones (clusters) en los datos con características similares?\n",
    "¿Se deberían normalizar las imágenes para visualizarlas mejor?\n",
    "¿Hay desequilibrio en las clases de la variable objetivo?\n",
    "Deberán contar con un repositorio en GitHubLinks to an external site., para compartir los resultados con el equipo docente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uta30eJkwMkn"
   },
   "source": [
    "***Importar Librerias***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Uta30eJkwMkn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "import plotly.express as px\n",
    "import random\n",
    "import functools\n",
    "\n",
    "from datetime_truncate import truncate\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from scipy.signal import periodogram\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from deap import base, creator, tools, algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IM6BKQlYHzZ7"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '99_Datasets/'# this needs to be changed to the directory of the excel files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMZNNul3pFT5"
   },
   "source": [
    "***Consolidación de datos (Opcional)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EypGPWR3gyn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cost_files_db = [f'{path}Compras 2022 V2.xlsx', f'{path}Compras 2023 V2.xlsx', f'{path}Compras 2024 V2.xlsx']\n",
    "df_cost = pd.DataFrame()\n",
    "\n",
    "for file in cost_files_db:\n",
    "    year_db_cost = pd.read_excel(file, skiprows = 4)\n",
    "    df_cost = pd.concat([df_cost, year_db_cost], ignore_index = True)\n",
    "\n",
    "df_cost.to_pickle(f'{path}base_compras_combinada_v2.pkl')\n",
    "\n",
    "\n",
    "sales_files_db = [f'{path}Ventas 2022 V2.xlsx', f'{path}Ventas 2023 V2.xlsx', f'{path}Ventas 2024 V2.xlsx']\n",
    "df_sales = pd.DataFrame()\n",
    "\n",
    "for file in sales_files_db:\n",
    "    year_db_sales = pd.read_excel(file, skiprows = 4)\n",
    "    df_sales = pd.concat([df_sales, year_db_sales], ignore_index = True)\n",
    "\n",
    "df_sales.to_pickle(f'{path}base_ventas_combinada_v2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Carga de Bases***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este proyecto tenemos 2 fuentes de información primarias que se obtuvieron directamente de los sistemas ERP del Grupo Golden\n",
    "\n",
    "**Transacciones de Ventas**: Considera todos los despachos ejecutados en una gasolinera de grupo golden, incluyendo la distinción de volumenes de compra así como datos adicionales.\n",
    "\n",
    "**Transacciones de Compra**: Considera todas las operaciones de compra de combustible para reabastecer a la gasolinera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "gnxMbCC93mxm",
    "outputId": "4a369161-4209-4aac-923f-5060e7f9af16"
   },
   "outputs": [],
   "source": [
    "df_sales = pd.read_pickle(f'{path}base_ventas_combinada_v2.pkl')\n",
    "df_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_purchases = pd.read_pickle(f'{path}base_compras_combinada_v2.pkl')\n",
    "df_purchases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Información Básica Fuentes de Información***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disponemos de una base de ventas de 15 variables. También tenemos dos variables (Fecha y Hora) las cuales deben ser cambiadas a formato DATETIME para poder analizar componentes temporales. En cuanto a la variable Posición, que es una variable categórica y tiene formato de una variable numérica, debe ser cambiada para reflejar el verdadero tipo de dato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.rename(columns = {'Folio':'folio_venta', 'Precio de Venta del litro con impuestos':'precio_bruto', 'Precio de Venta del litro sin impuestos':'precio_neto', 'Venta Ticket (con impuestos)':'venta_bruta', 'Venta sin impuestos':'venta_neta', 'Venta Unidades':'volumen_despachado', 'Producto':'producto'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disponemos de una base de compras de 14 variables. También tenemos dos variables (Fecha y Hora) las cuales deben ser cambiadas a formato DATETIME para poder unir ambas bases. De la misma manera tenemos datos a nivel factura asi como nivel unitarios. Para determinar el margen bruto de cada operación de despacho, nuestro interes es el costo unitario del combustible por lo que los campos IVA F, IEPS F, Sin Imp F y Precio Factura los descartaremos del data frame más adelante. En cuanto a la variable Tanque, que es una variable categórica, debe ser eliminada del análisis dado que solo es un indicador de donde fue depositado el combustible por lo que no agregara valor al análisis más adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_purchases.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_purchases.drop(['Precio Factura', 'IVA F', 'IEPS F', 'Sin imp F', 'Tanque'], axis = 1, inplace = True)\n",
    "df_purchases.rename(columns = {'Folio':'folio_compra','Por litro U':'costo_bruto', 'Sin imp U':'costo_neto', 'Producto':'producto'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Transformación de Bases***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para consolidar las bases de datos es importante garantizar que todos los campos de fecha existentes sean los correctos. En los siguientes pasos se crearan las estampas de tiempo que nos permitiran hacer este proceso, de la misma manera nos garantizará que podamos hacer análisis de temporalidad hacia adelante. De la misma manera se eliminaran los campos de Fecha y Hora dado que todas las operaciones hacia adelante se buscaran hacer con la estampa de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales['sale_date'] = pd.to_datetime(df_sales['Fecha'], errors = 'coerce', infer_datetime_format = True)\n",
    "df_sales['sale_localtime'] = pd.to_timedelta(df_sales['Hora'].astype(str))\n",
    "df_sales['sale_timeStamp'] = df_sales['sale_date'] + df_sales['sale_localtime']\n",
    "\n",
    "df_sales.drop(['Fecha', 'Hora'], axis = 1, inplace = True)\n",
    "df_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_purchases['purchase_date'] = pd.to_datetime(df_purchases['Fecha'], errors = 'coerce', infer_datetime_format = True)\n",
    "df_purchases['purchase_localtime'] = pd.to_timedelta(df_purchases['Hora'].astype(str))\n",
    "df_purchases['purchase_timeStamp'] = df_purchases['purchase_date'] + df_purchases['purchase_localtime']\n",
    "\n",
    "df_purchases.drop(['Fecha', 'Hora'], axis = 1, inplace = True)\n",
    "df_purchases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXQr9fPWo3AL"
   },
   "source": [
    "***Información datos faltantes o NA***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, los datos de venta principales como la fecha de transacción, cantidad de combustible, tipo de combustible, precio, entre otros, no registran valores faltantes. La gran mayoría de datos faltantes se observan en información del cliente como tipo de vehículo, el nombre, placas, etc. En este caso, los patrones de ausencia se deben a la falta de recolección o estandarización de este tipo de datos por parte de la gasolinera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculando data faltante en la base de datos por columna\n",
    "missing_percentage_per_column_sales = (df_sales.isnull().mean() * 100).round(2)\n",
    "print(\"Presencia de datos faltantes por dimensión (en porcentaje):\")\n",
    "print(missing_percentage_per_column_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, los datos de compras principales como la fecha de transacción, cantidad de combustible, tipo de combustible, precio, entre otros, no registran valores faltantes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculando data faltante en la base de datos por columna\n",
    "missing_percentage_per_column_purchases = (df_purchases.isnull().mean() * 100).round(2)\n",
    "print(\"Presencia de datos faltantes por dimensión (en porcentaje):\")\n",
    "print(missing_percentage_per_column_purchases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Unificación de Bases***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando las estampas de tiempo obtendremos el costo de reposición de cada despacho a cliente. Esto nos permitirá obtener un cálculo de margen bruto que al finalizar estará atado con nuestro objetivo final de optimización de precios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.sort_values(['producto', 'sale_date'], inplace = True)\n",
    "df_purchases.sort_values(['producto', 'purchase_date'], inplace = True)\n",
    "\n",
    "df_master = pd.DataFrame()\n",
    "\n",
    "for product_type in df_sales['producto'].unique():\n",
    "\n",
    "    sales_temp = df_sales[df_sales['producto'] == product_type]\n",
    "    purchases_temp = df_purchases[df_purchases['producto'] == product_type]\n",
    "    \n",
    "    merged_temp = pd.merge_asof(sales_temp, purchases_temp, left_on = 'sale_date', right_on = 'purchase_date',\n",
    "                                by = 'producto', direction = 'backward')\n",
    "\n",
    "    # Append the result to the main DataFrame\n",
    "    df_master = pd.concat([df_master, merged_temp], ignore_index = True)\n",
    "\n",
    "df_master.head(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Información Básica Data Frame***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSP6Fxa_pjb6"
   },
   "source": [
    "***Estadísticas de las variables***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sxehTS5RqPcN",
    "outputId": "5cc170ee-1e1a-450d-aaba-418e5d946d2b"
   },
   "outputs": [],
   "source": [
    "unique_values_per_column = df_master.nunique()\n",
    "unique_values_per_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hAJ2X4T6q-Qa",
    "outputId": "184708d8-9392-4a64-862d-1f9696e27130"
   },
   "outputs": [],
   "source": [
    "print('Identificación de manguera dispensadora:')\n",
    "print(df_master['Posición'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fSJzFMtV1aYc",
    "outputId": "f123a071-79c7-4d44-a5dc-67508a768a0b"
   },
   "outputs": [],
   "source": [
    "print('Tipo de Combustible vendido:')\n",
    "print(df_master['producto'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Afg8L7Q_sSCg"
   },
   "source": [
    "***Presencia de valores atípicos***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestras variables numéricas son Volumen, Precio Neto, Precio Bruto, Costo Neto, Costo Bruto, Venta Bruta. En estos gráficos de Caja y Densidad podemos observar la presencia de valores atípicos en Cantidad e Importe. Estos datos atípicos reflejan valores demasiado altos, lo cual resulta en que exista una distribución sesgada a la izquierda. Se debería eliminar estos registros para observar la nueva distribución.\n",
    "\n",
    "En el caso de Precio, podemos observar que existe una distribución ligeramente sesgada a la izquierda, con la presencia de pocos valores atípicos, pero que se encuentran en valores lógicos, a comparación de los valores atípicos en las otras variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jzmy_VEKsQTz",
    "outputId": "6c370e28-3937-4e9a-b777-70c3ba9ff257"
   },
   "outputs": [],
   "source": [
    "columnas_numericas = ['volumen_despachado', 'precio_neto', 'precio_bruto', 'costo_neto', 'costo_bruto','venta_bruta']\n",
    "\n",
    "for column in columnas_numericas:\n",
    "    plt.figure()\n",
    "    df_master[column].plot(kind = 'box')\n",
    "    plt.title(f'Gráfico de caja de {column}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "gldkAL0lxePL",
    "outputId": "444aba98-5a5e-4019-a2e8-464c9a9c172d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "\n",
    "for i, columna in enumerate(columnas_numericas, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    df_master[columna].plot(kind = 'density', color = 'skyblue')\n",
    "    plt.xlabel('Valor')\n",
    "    plt.ylabel('Densidad')\n",
    "    plt.title(f'Gráfico de Densidad de {columna}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tratamiento de valores atípicos***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZ4DYg-2x06o"
   },
   "source": [
    "En este caso podemos observar que existen valores atípicos que tal vez son producto de mal registro por parte de la Gasolinera, lo que resulta en valores demasiado altos. Por ejemplo, el valor máximo de *Cantidad* se ubica en 6 millones de galones por una sola venta (un vehículo promedio se llena con 12 galones). Por lo cual para filtrar estos datos atípicos, vamos a eliminar, solamente para este análisis exploratorio, el 10% de los datos más altos de la serie de *Cantidad*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "YtwPAOL41jJx",
    "outputId": "cc79b0e6-2c5b-4e02-ce54-f6339abc07d2"
   },
   "outputs": [],
   "source": [
    "df_master_filter = df_master[df_master['volumen_despachado'] <= df_master['volumen_despachado'].quantile(0.90)].copy()\n",
    "\n",
    "plt.figure(figsize = (12, 8))\n",
    "\n",
    "for i, columna in enumerate(columnas_numericas, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    df_master_filter[columna].plot(kind = 'density', color = 'skyblue')\n",
    "    plt.xlabel('Valor')\n",
    "    plt.ylabel('Densidad')\n",
    "    plt.title(f'Gráfico de densidad de {columna}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que filtramos los datos atípicos, observamos que las variables *Cantidad* e *Importe*, tienen una distribución similar, con un ligero sesgo a la izquierda. De la misma manera aprovecharemos para eliminar todos aquellos valores que no tengan datos faltantes. De la misma manera normalizamos a nivel hora dado que esto nos permitirá hacer un mejor análisis hacia adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master_filter.reset_index(drop = True, inplace = True)\n",
    "df_master_filter.drop('Cod.Externo', axis = 1, inplace = True)\n",
    "df_master_filter.dropna(axis = 0 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master_filter['sale_timeStamp'] = df_master_filter['sale_timeStamp'].dt.floor('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmap = {' DIESEL':'diesel', ' MAGNA':'magna', ' PREMIUM':'premium'}\n",
    "df_master_filter['producto'] = df_master_filter['producto'].map(pmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master_filter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master_filter.to_pickle(f'{path}base_master.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusiones Avance 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este análisis de la base de datos de nuestro Proyecto de Modelo para Optimización de Precios en Estaciones de Autoservicio tomamos como referencia la base diaria de ventas para el periodo 2022-2024 (abril). En primer lugar, consolidamos las bases de datos asegurándonos que cumplan con el tipo de dato correspondiente a la naturaleza de cada variable para poder analizar el contenido de la base de datos. \n",
    "\n",
    "Las variables completas hacen referencia a información de la transacción de Ventas de 1 gasolinera, en la cual encontramos 3 variables numéricas, 2 variables categóricas, 2 variables que hacen referencia a la fecha de la transacción y 1 variable de identificación de la transacción. \n",
    "Encontramos valores atípicos que pueden deberse a una incorrecta digitación de la información (por ejemplo, una venta fue de 6 millones de galones) por lo cual decidimos filtrar el 10% de los valores más altos de la variable Cantidad para poder realizar el análisis exploratorio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avance 2.Ingeniería de características**\n",
    "    \n",
    "Mayo 2024\n",
    "\n",
    "**Objetivos**\n",
    "    \n",
    "2.3 Crear nuevas características para mejorar el rendimiento de los modelos.\n",
    "\n",
    "2.4 Mitigar el riesgo de características sesgadas y acelerar la convergencia de algunos algoritmos.\n",
    "\n",
    "**Instrucciones**\n",
    "\n",
    "En esta fase, conocida como ingeniería de características (FE - Feature Engineering):\n",
    "\n",
    "Se aplicarán operaciones comunes para convertir los datos crudos del mundo real, en un conjunto de variables útiles para el aprendizaje automático. El procesamiento puede incluir:\n",
    "Generación de nuevas características\n",
    "Discretización o binning\n",
    "Codificación (ordinal, one hot,…)\n",
    "Escalamiento (normalización, estandarización, min – max,…)\n",
    "Transformación (logarítmica, exponencial, raíz cuadrada, Box – Cox, Yeo – Johnson,…)\n",
    "* Todas las decisiones y técnicas empleadas deben ser justificadas.\n",
    "\n",
    "\n",
    "Además, se utilizarán métodos de filtrado para la selección de características y técnicas de extracción de características, permitiendo reducir los requerimientos de almacenamiento, la complejidad del modelo y el tiempo de entrenamiento. Los ejemplos siguientes son ilustrativos, pero no exhaustivos, de lo que se podría aplicar:\n",
    "Umbral de varianza\n",
    "Correlación\n",
    "Chi-cuadrado\n",
    "ANOVA\n",
    "Análisis de componentes principales (PCA)\n",
    "Análisis factorial (FA)\n",
    "* Es necesario fundamentar los métodos ejecutados.\n",
    "\n",
    "Incluir conclusiones de la fase de \"Preparación de los datos\" en el contexto de la metodología CRISP-ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Análisis básico***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master_filter = pd.read_pickle(f'{path}base_master.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_info(df_interest, timeStamp): \n",
    "    df_interest['hour'] = df_interest[timeStamp].dt.hour\n",
    "    df_interest['month'] = df_interest[timeStamp].dt.month\n",
    "    df_interest['year'] = df_interest[timeStamp].dt.year\n",
    "    df_interest['dow_n'] = df_interest[timeStamp].dt.dayofweek\n",
    "    \n",
    "    dmap = {0:'Mon', 1:'Tue', 2:'Wed', 3:'Thu', 4:'Fri', 5:'Sat', 6:'Sun'}\n",
    "    \n",
    "    df_interest['dow'] = df_interest['dow_n'].map(dmap)\n",
    "    df_interest['clean_date'] = df_interest[timeStamp].dt.floor('D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alt = df_master_filter.copy()\n",
    "date_info(df_alt,'sale_timeStamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = 'dow', hue = 'producto', data = df_alt[df_alt['year'] == 2023], palette = 'viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = 'month', hue = 'producto', data = df_alt[df_alt['year'] == 2023], palette = 'viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8917xrcosMG"
   },
   "source": [
    "***Transformación de datos para procesamiento por fecha***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que estaremos trabajando con análisis de regresión para estimar demandas es necesario construir todas las estampas de manera que nos permita identificar patrones de demanda específicos (i.e. temporalidades). Es por esto que tenemos que generar un proceso de sumarización, además de asegurar que tenemos datos para la frecuencia mínima decidida (i.e. día hora) dentro los dataframes. De la misma manera se calcula el margen bruto para futuros análisis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master_summary = df_master_filter.groupby(by = ['producto','sale_timeStamp']).agg({\n",
    "    'precio_bruto' : [('precio_bruto', 'mean')],\n",
    "    'precio_neto' : [('precio_neto', 'mean')],\n",
    "    'volumen_despachado' : [('volumen_despachado', 'sum'),('transaction_num', 'count')],\n",
    "    'venta_neta' : [('venta_neta', 'sum')],\n",
    "    'venta_bruta' : [('venta_bruta', 'sum')],\n",
    "    'costo_bruto' : [('costo_bruto', 'mean')],\n",
    "    'costo_neto' : [('costo_neto', 'mean')],\n",
    "    'purchase_timeStamp' : [('purchase_timeStamp', 'min')]\n",
    "}).reset_index()\n",
    "df_master_summary.columns = [col[1] if col[1] else col[0] for col in df_master_summary.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master_summary['margen_bruto'] = df_master_summary['venta_bruta'] - df_master_summary['costo_bruto'] * df_master_summary['volumen_despachado']\n",
    "df_master_summary['margen_neto'] = df_master_summary['venta_neta'] - df_master_summary['costo_neto'] * df_master_summary['volumen_despachado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = df_master_summary['sale_timeStamp'].min()\n",
    "end_date = df_master_summary['sale_timeStamp'].max()\n",
    "\n",
    "all_dates = pd.date_range(start = start_date, end = end_date, freq = 'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_product_type(product_type):\n",
    "    product_df = df_master_summary[df_master_summary['producto'] == product_type]\n",
    "    product_df = product_df.set_index('sale_timeStamp').reindex(all_dates).rename_axis('sale_timeStamp').reset_index()\n",
    "   \n",
    "    product_df['precio_bruto'] = product_df['precio_bruto'].ffill()\n",
    "    product_df['precio_neto'] = product_df['precio_neto'].ffill()\n",
    "    \n",
    "    product_df['costo_bruto'] = product_df['costo_bruto'].ffill()\n",
    "    product_df['costo_neto'] = product_df['costo_neto'].ffill()\n",
    "\n",
    "    product_df['purchase_timeStamp'] = product_df['purchase_timeStamp'].ffill()\n",
    "    \n",
    "    product_df['venta_neta'] = product_df['venta_neta'].fillna(0)\n",
    "    product_df['venta_bruta'] = product_df['venta_bruta'].fillna(0)\n",
    "\n",
    "    product_df['volumen_despachado'] = product_df['volumen_despachado'].fillna(0)\n",
    "    product_df['transaction_num'] = product_df['transaction_num'].fillna(0)\n",
    "    product_df['margen_bruto'] = product_df['margen_bruto'].fillna(0)\n",
    "    product_df['margen_neto'] = product_df['margen_neto'].fillna(0)\n",
    "\n",
    "    product_df['precio_bruto'] = product_df['precio_bruto'].fillna(0)\n",
    "    product_df['precio_neto'] = product_df['precio_neto'].fillna(0)\n",
    "    \n",
    "    product_df['costo_bruto'] = product_df['costo_bruto'].fillna(0)\n",
    "    product_df['costo_neto'] = product_df['costo_neto'].fillna(0)\n",
    "\n",
    "    product_df['purchase_timeStamp'] = product_df['purchase_timeStamp'].fillna(start_date)\n",
    "    \n",
    "    product_df['producto'] = product_type\n",
    "    \n",
    "    return product_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_types = df_master_summary['producto'].unique()\n",
    "product_dfs = {ptype: process_product_type(ptype) for ptype in product_types}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_magna = product_dfs['magna']\n",
    "df_premium = product_dfs['premium']\n",
    "df_diesel = product_dfs['diesel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product_time = pd.concat([df_magna, df_premium,df_diesel], ignore_index = True)\n",
    "date_info(df_product_time,'sale_timeStamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product_time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvpN5Chjx2wo"
   },
   "source": [
    "***Análisis de Temporalidad General***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se analizó de manera general si existe un patrón de demanda en algún periodo en particular. Parte de los hallagazgos fueron que la demanda de la gasolina magna ha disminuido significativamente desde finales de 2022, mientras que la venta del combustible PREMIUM muestra una ligera tendencia creciente en el mismo periodo. Esto genera una incógnita sobre si esta caida obedece a un cámbio en la dinámica de precios o si se debe a un competidor nuevo cercano a dicha sucursal o un factor exógeno del que el equipo no tenga conocimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries = df_product_time[['clean_date', 'hour', 'month', 'year', 'dow', 'volumen_despachado', 'producto','transaction_num']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "b2adE71yT1-7",
    "outputId": "6946eff8-0fa8-4dc8-8cb6-8706722f8d71"
   },
   "outputs": [],
   "source": [
    "grouped_date = df_timeseries.groupby(by = ['clean_date', 'producto']).sum().reset_index()\n",
    "grouped_dayHour = df_timeseries.drop('clean_date', axis = 1).groupby(by = ['dow', 'hour']).sum()['transaction_num'].unstack()\n",
    "grouped_dayMonth = df_timeseries.drop('clean_date', axis = 1).groupby(by = ['dow', 'month']).sum()['transaction_num'].unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14, 7))\n",
    "sns.lineplot(data = grouped_date, x = 'clean_date', y = 'volumen_despachado', hue = 'producto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general se puede observa que no existe actividad significativa los días domingos en esta sucursal lo cuál hace mucho sentido dado que es una sucursal que se encuentra al costado de una carretera que generalmente tiene la mayoría de su tráfico entre semana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oT7NJC2d06N"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,7))\n",
    "sns.heatmap(grouped_dayHour, cmap = 'viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,7))\n",
    "sns.clustermap(grouped_dayHour, cmap = 'coolwarm', method = 'centroid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma manera se puede observar que los primeros meses del año son aquellos que existe más tráfico en la sucursal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,7))\n",
    "sns.heatmap(grouped_dayMonth, cmap = 'viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,7))\n",
    "sns.clustermap(grouped_dayMonth, cmap = 'coolwarm', method = 'centroid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Análisis de Demanda***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se incluyen los componentes de demanda que nos permitirán decidir un modelo de predicción de demanda para cada uno de los productos que existen (Magna, Premium y Diesel). Estos factores son tendencia, temporalidad y residuales y nos ayudaran a identificar si existen ciclos continuos de demanda que faciliten el entrenamiento de nuestros algorítmos. Dentro de cada uno de los productos se puede visualizar que hay un alto componente de temporalidad que se tiene que considerar a la hora de construir el modelo, lo que es un indicativo que requeriremos los diferentes niveles de las estampas de tiempo (Hora, Dia, Mes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_magna.set_index('sale_timeStamp', inplace = True)\n",
    "df_magna.sort_index(inplace = True)\n",
    "df_premium.set_index('sale_timeStamp', inplace = True)\n",
    "df_premium.sort_index(inplace = True)\n",
    "df_diesel.set_index('sale_timeStamp', inplace = True)\n",
    "df_diesel.sort_index(inplace = True)\n",
    "\n",
    "#daily_magna_df = df_magna['Volumen despachado'].resample('D').sum()\n",
    "#daily_premium_df = df_premium['Volumen despachado'].resample('D').sum() \n",
    "#daily_diesel_df = df_diesel['Volumen despachado'].resample('D').sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Análisis de Demanda Magna***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(df_magna.index, pd.DatetimeIndex):\n",
    "    df_magna.index = pd.to_datetime(df_magna.index)\n",
    "\n",
    "print(\"Start date:\", df_magna.index[0])\n",
    "print(\"End date:\", df_magna.index[-1])\n",
    "print(\"Number of data points:\", len(df_magna))\n",
    "\n",
    "if df_magna.index[0] <= pd.Timestamp.max - pd.DateOffset(days = len(df_magna)):\n",
    "    if not df_magna.index.freq:\n",
    "        df_magna.index.freq = 'h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition_magna = sm.tsa.seasonal_decompose(df_magna['volumen_despachado'], model = 'additive')\n",
    "\n",
    "trend_magna = decomposition_magna.trend\n",
    "seasonal_magna = decomposition_magna.seasonal\n",
    "residual_magna = decomposition_magna.resid\n",
    "\n",
    "fig = decomposition_magna.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x = df_magna['volumen_despachado'].index, y = seasonal_magna, labels={'y': 'Seasonality', 'x': 'Date'}, title = 'Seasonality Component')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistical Summary of Trend Component:\")\n",
    "print(trend_magna.describe())\n",
    "\n",
    "print(\"\\nStatistical Summary of Seasonal Component:\")\n",
    "print(seasonal_magna.describe())\n",
    "\n",
    "print(\"\\nStatistical Summary of Residual Component:\")\n",
    "print(residual_magna.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = df_magna['volumen_despachado']\n",
    "print(\"Correlation with Original Series:\")\n",
    "print(\"Trend Correlation:\", original.corr(trend_magna))\n",
    "print(\"Seasonal Correlation:\", original.corr(seasonal_magna))\n",
    "print(\"Residual Correlation:\", original.corr(residual_magna))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies_magna, spectrum_magna = periodogram(seasonal_magna.dropna(), scaling = 'spectrum')\n",
    "plt.figure()\n",
    "plt.plot(frequencies_magna, spectrum_magna)\n",
    "plt.title('Power Spectrum of the Seasonal Component')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Spectral Power')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(residual_magna.dropna())\n",
    "plt.title('Autocorrelation of Residuals')\n",
    "plt.show()\n",
    "\n",
    "plot_pacf(residual_magna.dropna())\n",
    "plt.title('Partial Autocorrelation of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Análisis de Demanda Premium***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(df_premium.index, pd.DatetimeIndex):\n",
    "    df_premium.index = pd.to_datetime(df_premium.index)\n",
    "\n",
    "print(\"Start date:\", df_premium.index[0])\n",
    "print(\"End date:\", df_premium.index[-1])\n",
    "print(\"Number of data points:\", len(df_premium))\n",
    "\n",
    "if df_premium.index[0] <= pd.Timestamp.max - pd.DateOffset(days = len(df_premium)):\n",
    "    if not df_premium.index.freq:\n",
    "        df_premium.index.freq = 'h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition_premium = sm.tsa.seasonal_decompose(df_premium['volumen_despachado'], model = 'additive')\n",
    "\n",
    "trend_premium = decomposition_premium.trend\n",
    "seasonal_premium = decomposition_premium.seasonal\n",
    "residual_premium = decomposition_premium.resid\n",
    "\n",
    "fig = decomposition_premium.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x = df_premium['volumen_despachado'].index, y = seasonal_premium, labels={'y': 'Seasonality', 'x': 'Date'}, title = 'Seasonality Component')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistical Summary of Trend Component:\")\n",
    "print(trend_premium.describe())\n",
    "\n",
    "print(\"\\nStatistical Summary of Seasonal Component:\")\n",
    "print(seasonal_premium.describe())\n",
    "\n",
    "print(\"\\nStatistical Summary of Residual Component:\")\n",
    "print(residual_premium.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = df_premium['volumen_despachado']\n",
    "print(\"Correlation with Original Series:\")\n",
    "print(\"Trend Correlation:\", original.corr(trend_premium))\n",
    "print(\"Seasonal Correlation:\", original.corr(seasonal_premium))\n",
    "print(\"Residual Correlation:\", original.corr(residual_premium))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies_premium, spectrum_premium = periodogram(seasonal_premium.dropna(), scaling = 'spectrum')\n",
    "plt.figure()\n",
    "plt.plot(frequencies_premium, spectrum_premium)\n",
    "plt.title('Power Spectrum of the Seasonal Component')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Spectral Power')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(residual_premium.dropna())\n",
    "plt.title('Autocorrelation of Residuals')\n",
    "plt.show()\n",
    "\n",
    "plot_pacf(residual_premium.dropna())\n",
    "plt.title('Partial Autocorrelation of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Análisis de Demanda Diesel***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(df_diesel.index, pd.DatetimeIndex):\n",
    "    df_diesel.index = pd.to_datetime(df_diesel.index)\n",
    "\n",
    "print(\"Start date:\", df_diesel.index[0])\n",
    "print(\"End date:\", df_diesel.index[-1])\n",
    "print(\"Number of data points:\", len(df_diesel))\n",
    "\n",
    "if df_diesel.index[0] <= pd.Timestamp.max - pd.DateOffset(days = len(df_diesel)):\n",
    "    if not df_diesel.index.freq:\n",
    "        df_diesel.index.freq = 'h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition_diesel = sm.tsa.seasonal_decompose(df_diesel['volumen_despachado'], model = 'additive')\n",
    "\n",
    "trend_diesel = decomposition_diesel.trend\n",
    "seasonal_diesel = decomposition_diesel.seasonal\n",
    "residual_diesel = decomposition_diesel.resid\n",
    "\n",
    "fig = decomposition_diesel.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x = df_diesel['volumen_despachado'].index, y = seasonal_diesel, labels={'y': 'Seasonality', 'x': 'Date'}, title = 'Seasonality Component')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistical Summary of Trend Component:\")\n",
    "print(trend_diesel.describe())\n",
    "\n",
    "print(\"\\nStatistical Summary of Seasonal Component:\")\n",
    "print(seasonal_diesel.describe())\n",
    "\n",
    "print(\"\\nStatistical Summary of Residual Component:\")\n",
    "print(residual_diesel.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = df_diesel['volumen_despachado']\n",
    "print(\"Correlation with Original Series:\")\n",
    "print(\"Trend Correlation:\", original.corr(trend_diesel))\n",
    "print(\"Seasonal Correlation:\", original.corr(seasonal_diesel))\n",
    "print(\"Residual Correlation:\", original.corr(residual_diesel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies_diesel, spectrum_diesel = periodogram(seasonal_diesel.dropna(), scaling = 'spectrum')\n",
    "plt.figure()\n",
    "plt.plot(frequencies_diesel, spectrum_diesel)\n",
    "plt.title('Power Spectrum of the Seasonal Component')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Spectral Power')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(residual_diesel.dropna())\n",
    "plt.title('Autocorrelation of Residuals')\n",
    "plt.show()\n",
    "\n",
    "plot_pacf(residual_diesel.dropna())\n",
    "plt.title('Partial Autocorrelation of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_magna.to_pickle(f'{path}base_magna.pkl')\n",
    "df_premium.to_pickle(f'{path}base_premium.pkl')\n",
    "df_diesel.to_pickle(f'{path}base_diesel.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identificación de Factores Exógenos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un factor importante para determinar el precio de venta de gasolina es el precio del petróleo del cual se derivó la gasolina que se venderá. Para esto tomamos la base del EIA (US Energy Information Administration) del precio de petróleo BRENT (referencia para México). En esta sección analizaremos la relación del precio del petróleo con los precios de compra (es decir los precios a los cuales las gasolineras adquirieron el producto). Debido a que el precio internacional del petróleo no tiene un efecto inmediato sobre el precio de los proveedores, se debe realizar un análisis de correlación con rezago, para poder determinar el impacto del precio del petróleo en los precios de distribución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_master = pd.read_pickle(f'{path}base_master.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_petroleo = pd.read_csv(f'{path}Europe_Brent_Spot_Price_FOB.csv', header = None)\n",
    "db_petroleo.columns = ['fecha', 'precio_brent']\n",
    "db_petroleo['market_date'] = pd.to_datetime(db_petroleo['fecha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_days = pd.date_range(start = db_petroleo['market_date'].min(), end = db_petroleo['market_date'].max(), freq = 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_petroleo = db_petroleo.set_index('market_date').reindex(all_days).ffill()\n",
    "db_petroleo = db_petroleo.reset_index()\n",
    "db_petroleo = db_petroleo.drop(index = 0).reset_index(drop = True)\n",
    "db_petroleo = db_petroleo.rename(columns = {'index': 'market_date'})\n",
    "ppetroleo = db_petroleo[db_petroleo['market_date'] > '2021-12-30']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Básicamente podemos observar que existe una correlación visual entre el precio del petróleo Brent y el precio neto de compra al cual la gasolinera adquiere los derivados de petróleo. Con esta información visual, haremos un análisis de correlación con rezago para determinar el rezago óptimo del precio del petróleo con el costo de adquisición por tipo de combustible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_combustible = base_master.loc[base_master['producto'] == 'magna']\n",
    "\n",
    "precio_promedio_diario = base_combustible.groupby('purchase_date')['costo_neto'].mean().reset_index()\n",
    "\n",
    "precio_promedio_diario.columns = ['purchase_date', 'costo_neto']\n",
    "\n",
    "merged_db_price_cost = pd.merge(precio_promedio_diario, ppetroleo, left_on = 'purchase_date', right_on = 'market_date', how = 'inner')\n",
    "\n",
    "data_p_c = merged_db_price_cost[['purchase_date', 'costo_neto', 'market_date', 'precio_brent']]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Fecha')\n",
    "ax1.set_ylabel('Precio Brent', color = color)\n",
    "ax1.plot(data_p_c['market_date'], data_p_c['precio_brent'], color = color, label = 'Precio Brent')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('Costo Neto', color = color)\n",
    "ax2.plot(data_p_c['market_date'], data_p_c['costo_neto'], color=color, label='Costo Neto')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Título y leyenda\n",
    "plt.title('Costo Neto Promedio Diario MAGNA vs Precio diario del Petróleo Brent')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combustibles = ['magna', 'premium', 'diesel']\n",
    "\n",
    "for combustible in combustibles:\n",
    "    base_combustible = base_master.loc[base_master['producto'] == combustible]\n",
    "\n",
    "    precio_promedio_diario = base_combustible.groupby('purchase_date')['costo_neto'].mean().reset_index()\n",
    "    precio_promedio_diario.columns = ['purchase_date', 'costo_neto']\n",
    "    merged_data = pd.merge(precio_promedio_diario, ppetroleo, left_on = 'purchase_date', right_on = 'market_date', how = 'inner')\n",
    "    \n",
    "    data = merged_data[['purchase_date', 'costo_neto', 'market_date', 'precio_brent']]\n",
    "\n",
    "    lag_acf = plot_acf(data['precio_brent'], lags = 45)\n",
    "  \n",
    "    plt.xlabel('Rezago (días)')\n",
    "    plt.ylabel('Autocorrelación')\n",
    "    plt.title(f'Autocorrelación del precio del petróleo Brent {combustible}')\n",
    "    plt.show()\n",
    "\n",
    "  # Calcular la correlación entre el precio del petróleo Brent y el costo neto con diferentes rezagos\n",
    "    correlation_results = {}\n",
    "    for lag in range(1, 46):\n",
    "        data['precio_brent_lagged'] = data['precio_brent'].shift(lag)\n",
    "        correlation = data[['precio_brent_lagged', 'costo_neto']].corr().iloc[0, 1]\n",
    "        correlation_results[lag] = correlation\n",
    "\n",
    "\n",
    "    plt.plot(list(correlation_results.keys()), list(correlation_results.values()), marker = 'o')\n",
    "    plt.xlabel('Rezago (días)')\n",
    "    plt.ylabel('Correlación')\n",
    "    plt.title(f'Correlación entre el precio del petróleo Brent y el precio neto - {combustible}')\n",
    "    plt.show()\n",
    "\n",
    "    optimal_lag = max(correlation_results, key = lambda key: abs(correlation_results[key]))\n",
    "\n",
    "    print(\"Rezago óptimo:\", optimal_lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras el análisis de correlación con rezago pudimos determinar que existen diferentes rezagos óptimos para cada tipo de combustible. Para MAGNA, el rezago óptimo es de 2 días, es decir, el precio del petróleo con un rezago de 2 días tiene mayor correlación con el costo de adquisición. Para PREMIUM, el rezago óptimo es de 17 días y para DIESEL, el rezago óptimo es de 8 días. Esta diferencia en rezagos puede deberse a la naturaleza del tipo de combustible.  Debido a que encontramos esta correlación, debemos agregar la variable del precio del petróleo a nuestra base de datos. Para esto agregaremos el precio del petróleo con su rezago óptimo por cada tipo de combustible para cada data frame de combustible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_magna = pd.read_pickle(f'{path}base_magna.pkl')\n",
    "df_premium = pd.read_pickle(f'{path}base_premium.pkl')\n",
    "df_diesel = pd.read_pickle(f'{path}base_diesel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_magna['purchase_timeStamp'] = df_magna['purchase_timeStamp'].dt.floor('D')\n",
    "df_premium['purchase_timeStamp'] = df_premium['purchase_timeStamp'].dt.floor('D')\n",
    "df_diesel['purchase_timeStamp'] = df_diesel['purchase_timeStamp'].dt.floor('D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rezago_magna = 2\n",
    "rezago_premium = 17\n",
    "rezago_diesel = 8\n",
    "\n",
    "ppetroleo_magna = db_petroleo.copy()\n",
    "ppetroleo_magna['market_date'] = ppetroleo_magna['market_date'] + pd.DateOffset(days = rezago_magna)\n",
    "df_magna = pd.merge(df_magna.reset_index(), ppetroleo_magna.drop(['fecha'], axis = 1), left_on = 'purchase_timeStamp', right_on = 'market_date', how = 'inner')\n",
    "df_magna.drop('market_date', axis = 1, inplace = True)\n",
    "date_info(df_magna,'sale_timeStamp')\n",
    "\n",
    "ppetroleo_premium = db_petroleo.copy()\n",
    "ppetroleo_premium['market_date'] = ppetroleo_premium['market_date'] + pd.DateOffset(days = rezago_premium)\n",
    "df_premium = pd.merge(df_premium.reset_index(), ppetroleo_premium.drop(['fecha'], axis = 1), left_on = 'purchase_timeStamp', right_on = 'market_date', how = 'inner')\n",
    "df_premium.drop('market_date', axis = 1, inplace = True)\n",
    "date_info(df_premium,'sale_timeStamp')\n",
    "\n",
    "ppetroleo_diesel = db_petroleo.copy()\n",
    "ppetroleo_diesel['market_date'] = ppetroleo_diesel['market_date'] + pd.DateOffset(days = rezago_diesel)\n",
    "df_diesel = pd.merge(df_diesel.reset_index(), ppetroleo_diesel.drop(['fecha'], axis = 1), left_on = 'purchase_timeStamp', right_on = 'market_date', how = 'inner')\n",
    "df_diesel.drop('market_date', axis = 1, inplace = True)\n",
    "date_info(df_diesel,'sale_timeStamp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_magna.to_pickle(f'{path}base_magna_final.pkl')\n",
    "df_premium.to_pickle(f'{path}base_premium_final.pkl')\n",
    "df_diesel.to_pickle(f'{path}base_diesel_final.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusiones Avance 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nuestro modelo de optimización de precios la preparación de datos es el paso más importante previo al modelamiento debido a que logramos comprender la base de datos disponible por parte de las gasolineras, al igual que el contexto económico del negocio y las variables que afectan las ventas de combustible. \n",
    "\n",
    "En la comprensión de la base de datos, identificamos variables de oferta y demanda de las gasolineras al igual que obtuvimos nuestra variable dependiente para nuestro modelo que es el Margen de Ganancias. De igual manera, obtuvimos información sobre el comportamiento temporal de nuestras variables para extraer insights sobre el funcionamiento del negocio y su estado actual (como mencionamos anteriormente, encontramos una reducción en el volumen de combustible vendido a lo largo del periodo 2022- 2024). También ligamos el comportamiento de los costos de combustibles al precio internacional del petróleo, de tal manera que obtuvimos una variable externa que nos otorga bastante información que puede ser útil en el modelamiento de optimización de precios.\n",
    "\n",
    "También comprendimos que es necesaria la división de nuestra base de datos en tipo de Combustible ofertado. Esto debido a que la estacionalidad y tendencia de ventas varía de acuerdo con el tipo de combustible, al igual que el precio del petróleo influye de diferente manera a cada tipo de combustible. Por lo cual, en lugar de crear variables dummies para cada tipo de combustible, vamos a crear un modelo especifico para cada tipo de combustible disponible en nuestra base de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avance 3.Ingeniería de características**\n",
    "    \n",
    "Mayo 2024\n",
    "\n",
    "**Objetivos**\n",
    "    \n",
    "3.1 Establecer las medidas de calidad del modelo de aprendizaje automático.\r\n",
    "\r\n",
    "3.2 Proporcionar un marco de referencia para evaluar y mejorar modelos más avanzado\n",
    ".\n",
    "\n",
    "**Instruccion***\n",
    "\n",
    "Este avance consiste en construir un modelo de referencia que permita evaluar la viabilidad del problema. Si el baseline tiene un rendimiento similar al azar, podría indicar que el problema es intrínsecamente difícil o que los datos no contienen suficiente información para predecir el objetivo. De lo contrario, el baseline podría como una solución mínima aceptable cuando se trabaja en escenarios donde incluso un modelo simple puede proporcionar valor.\r\n",
    "\r\n",
    "Un baseline facilita también la gestión de expectativas, tanto dentro del equipo como con los stakeholders, pues proporciona una comprensión inicial de lo que se puede lograr con métodos simples antes de invertir tiempo y recursos en enfoques más complejos.\r\n",
    "\r\n",
    "Las siguientes son algunas de las preguntas que deberán abordar durante esta fase:\r\n",
    "\r\n",
    "¿Qué algoritmo se puede utilizar como baseline para predecir las variables objetivo? \r\n",
    "¿Se puede determinar la importancia de las características para el modelo generado? Recuerden que incluir características irrelevantes puede afectar negativamente el rendimiento del modelo y aumentar la complejidad sin beneficios sustanciales.\r\n",
    "¿El modelo está sub/sobreajustando los datos de entrenamiento?\r\n",
    "¿Cuál es la métrica adecuada para este problema de negocio? \r\n",
    "¿Cuál debería ser el desempeño mínimo obtener?\r\n",
    " o real, en un conjunto de variables útiles para el aprendizaje automático. El procesamiento puede incluir:\n",
    "Generación de nuevas características\n",
    "Discretización o binning\n",
    "Codificación (ordinal, one hot,…)\n",
    "Escalamiento (normalización, estandarización, min – max,…)\n",
    "Transformación (logarítmica, exponencial, raíz cuadrada, Box – Cox, Yeo – Johnson,…)\n",
    "* Todas las decisiones y técnicas empleadas deben ser justificadas.\n",
    "\n",
    "\n",
    "Además, se utilizarán métodos de filtrado para la selección de características y técnicas de extracción de características, permitiendo reducir los requerimientos de almacenamiento, la complejidad del modelo y el tiempo de entrenamiento. Los ejemplos siguientes son ilustrativos, pero no exhaustivos, de lo que se podría aplicar:\n",
    "Umbral de varianza\n",
    "Correlación\n",
    "Chi-cuadrado\n",
    "ANOVA\n",
    "Análisis de componentes principales (PCA)\n",
    "Análisis factorial (FA)\n",
    "* Es necesario fundamentar los métodos ejecutados.\n",
    "\n",
    "Incluir conclusiones de la fase de \"Preparación de los datos\" en el contexto de la metodología CRISP-ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_magna = pd.read_pickle(f'{path}base_magna_final.pkl')\n",
    "df_premium = pd.read_pickle(f'{path}base_premium_final.pkl')\n",
    "df_diesel = pd.read_pickle(f'{path}base_diesel_final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_magna_alt = df_magna[['volumen_despachado', 'precio_neto']].copy()\n",
    "original_type_a = df_magna['volumen_despachado'].dtype\n",
    "original_type_b = df_magna['precio_neto'].dtype\n",
    "\n",
    "df_magna_alt.replace(0, pd.NA, inplace = True)\n",
    "df_magna_alt.dropna(inplace = True)\n",
    "\n",
    "df_magna_alt['volumen_despachado'] = df_magna_alt['volumen_despachado'].astype(original_type_a)\n",
    "df_magna_alt['precio_neto'] = df_magna_alt['volumen_despachado'].astype(original_type_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_premium_alt = df_premium[['volumen_despachado', 'precio_neto']].copy()\n",
    "original_type_a = df_premium['volumen_despachado'].dtype\n",
    "original_type_b = df_premium['precio_neto'].dtype\n",
    "\n",
    "df_premium_alt.replace(0, pd.NA, inplace = True)\n",
    "df_premium_alt.dropna(inplace = True)\n",
    "\n",
    "df_premium_alt['volumen_despachado'] = df_premium_alt['volumen_despachado'].astype(original_type_a)\n",
    "df_premium_alt['precio_neto'] = df_premium_alt['volumen_despachado'].astype(original_type_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diesel_alt = df_diesel[['volumen_despachado', 'precio_neto']].copy()\n",
    "original_type_a = df_diesel['volumen_despachado'].dtype\n",
    "original_type_b = df_diesel['precio_neto'].dtype\n",
    "\n",
    "df_diesel_alt.replace(0, pd.NA, inplace = True)\n",
    "df_diesel_alt.dropna(inplace = True)\n",
    "\n",
    "df_diesel_alt['volumen_despachado'] = df_diesel_alt['volumen_despachado'].astype(original_type_a)\n",
    "df_diesel_alt['precio_neto'] = df_diesel_alt['volumen_despachado'].astype(original_type_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_magna_alt['log_price'] = np.log(df_magna_alt['precio_neto'])\n",
    "df_magna_alt['log_demand'] = np.log(df_magna_alt['volumen_despachado'])\n",
    "\n",
    "df_premium_alt['log_price'] = np.log(df_premium_alt['precio_neto'])\n",
    "df_premium_alt['log_demand'] = np.log(df_premium_alt['volumen_despachado'])\n",
    "\n",
    "df_diesel_alt['log_price'] = np.log(df_diesel_alt['precio_neto'])\n",
    "df_diesel_alt['log_demand'] = np.log(df_diesel_alt['volumen_despachado'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_elasticity_magna = LinearRegression()\n",
    "model_lr_elasticity_magna.fit(df_magna_alt[['log_price']], df_magna_alt[['log_demand']])\n",
    "price_elasticity_magna = model_lr_elasticity_magna.coef_[0]\n",
    "\n",
    "model_lr_elasticity_premium = LinearRegression()\n",
    "model_lr_elasticity_premium.fit(df_premium_alt[['log_price']], df_premium_alt[['log_demand']])\n",
    "price_elasticity_premium = model_lr_elasticity_premium.coef_[0]\n",
    "\n",
    "model_lr_elasticity_diesel = LinearRegression()\n",
    "model_lr_elasticity_diesel.fit(df_diesel_alt[['log_price']], df_diesel_alt[['log_demand']])\n",
    "price_elasticity_diesel = model_lr_elasticity_diesel.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_magna = df_magna[['precio_neto', 'costo_neto', 'precio_brent', 'hour', 'month', 'dow_n']].copy()\n",
    "target_magna = df_magna['volumen_despachado'].copy()\n",
    "\n",
    "features_premium = df_premium[['precio_neto', 'costo_neto', 'precio_brent', 'hour', 'month', 'dow_n']].copy()\n",
    "target_premium = df_premium['volumen_despachado'].copy()\n",
    "\n",
    "features_diesel = df_diesel[['precio_neto', 'costo_neto', 'precio_brent', 'hour', 'month', 'dow_n']].copy()\n",
    "target_diesel = df_diesel['volumen_despachado'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df_interest):\n",
    "    df_interest['hour_sin'] = np.sin(2 * np.pi * df_interest['hour']/23.0)\n",
    "    df_interest['hour_cos'] = np.cos(2 * np.pi * df_interest['hour']/23.0)\n",
    "    df_interest['day_sin'] = np.sin(2 * np.pi * df_interest['dow_n']/6.0)\n",
    "    df_interest['day_cos'] = np.cos(2 * np.pi * df_interest['dow_n']/6.0)\n",
    "    df_interest['month_sin'] = np.sin(2 * np.pi * df_interest['month']/11.0)\n",
    "    df_interest['month_cos'] = np.cos(2 * np.pi * df_interest['month']/11.0)\n",
    "    df_interest.drop(['hour', 'dow_n', 'month'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering(features_magna)\n",
    "feature_engineering(features_premium)\n",
    "feature_engineering(features_diesel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_scaled_magna = scaler.fit_transform(features_magna)\n",
    "features_scaled_premium = scaler.fit_transform(features_premium)\n",
    "features_scaled_diesel = scaler.fit_transform(features_diesel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_magna, X_test_magna, y_train_magna, y_test_magna = train_test_split(features_scaled_magna, target_magna, test_size = 0.2, random_state = 42)\n",
    "X_train_premium, X_test_premium, y_train_premium, y_test_premium = train_test_split(features_scaled_premium, target_premium, test_size = 0.2, random_state = 42)\n",
    "X_train_diesel, X_test_diesel, y_train_diesel, y_test_diesel = train_test_split(features_scaled_diesel, target_diesel, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_magna = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "model_premium = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "model_diesel = RandomForestRegressor(n_estimators = 100, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(random_state=42)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_magna.fit(X_train_magna, y_train_magna)\n",
    "model_premium.fit(X_train_premium, y_train_premium)\n",
    "model_diesel.fit(X_train_diesel, y_train_diesel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demand Prediction Model MSE (Magna): 24260.906214291863\n",
      "Demand Prediction Model MSE (Premium): 1659.8372604914277\n",
      "Demand Prediction Model MSE (Diesel): 1059.0441196169459\n"
     ]
    }
   ],
   "source": [
    "y_pred_magna = model_magna.predict(X_test_magna)\n",
    "print('Demand Prediction Model MSE (Magna):', mean_squared_error(y_test_magna, y_pred_magna))\n",
    "\n",
    "y_pred_premium = model_premium.predict(X_test_premium)\n",
    "print('Demand Prediction Model MSE (Premium):', mean_squared_error(y_test_premium, y_pred_premium))\n",
    "\n",
    "y_pred_diesel = model_diesel.predict(X_test_diesel)\n",
    "print('Demand Prediction Model MSE (Diesel):', mean_squared_error(y_test_diesel, y_pred_diesel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optmización de Modelo: Modelo Genómico**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator.create('FitnessMax', base.Fitness, weights = (1.0,))\n",
    "creator.create('Individual', list, fitness = creator.FitnessMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolbox_magna = base.Toolbox()\n",
    "toolbox_magna.register('attr_float', random.uniform, 15, 30)  # Price range\n",
    "toolbox_magna.register('individual', tools.initRepeat, creator.Individual, toolbox_magna.attr_float, n = 1)\n",
    "toolbox_magna.register('population', tools.initRepeat, list, toolbox_magna.individual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalGrossMargin(individual, train_data, model_type, feature_names, current_price, elasticity):\n",
    "    try:\n",
    "        # Price from the individual\n",
    "        price = individual[0]\n",
    "\n",
    "        # Calculate mean values of each feature in train_data\n",
    "        relevant_columns = [col for col in feature_names if col != 'price']  # Assuming 'price' should not be included in the mean calculation\n",
    "        mean_values = train_data[relevant_columns].mean()\n",
    "\n",
    "        msg_1 = \"Data Type of mean_values:\" + type(mean_values) # Debugging\n",
    "        msg_2 = \"Mean Values: \"+ mean_values  # Debugging\n",
    "\n",
    "        arcpy.AddMessage(msg_1)\n",
    "        arcpy.AddMessage(msg_2)\n",
    "        if not isinstance(mean_values, pd.Series):\n",
    "            raise ValueError(\"mean_values should be a pandas Series, check the train_data and relevant_columns\")\n",
    "\n",
    "        # Construct the data array by inserting the price into the correct position\n",
    "        # If 'price' is supposed to be the first feature, then it's correctly placed at index 0\n",
    "        data_array = np.insert(mean_values.values, 0, price)  # mean_values should be ordered as per feature_names minus 'price'\n",
    "\n",
    "#        print(\"Data Array:\", data_array)  # Debugging\n",
    "        \n",
    "        if len(data_array) != len(feature_names):\n",
    "            raise ValueError(f\"Data array length ({len(data_array)}) does not match feature names length ({len(feature_names)})\")\n",
    "\n",
    "        # Create a DataFrame for the prediction\n",
    "        temp_features = pd.DataFrame([data_array], columns=feature_names)\n",
    "        \n",
    "        # Scaling the features\n",
    "        scaled_features = scaler.transform(temp_features)\n",
    "\n",
    "        # Predicting the demand using the scaled features\n",
    "        predicted_demand = model_type.predict(scaled_features)[0]\n",
    "\n",
    "        # Accessing the cost of goods sold from the temp_features DataFrame\n",
    "        cost_of_goods_sold = temp_features['costo_neto'].iloc[0]\n",
    "\n",
    "        # Calculating the gross margin\n",
    "        gross_margin = (price - cost_of_goods_sold) * predicted_demand\n",
    "\n",
    "        # Adding a penalty for large price deviations based on elasticity\n",
    "        penalty_factor = 10\n",
    "        penalty = penalty_factor * (abs(price - current_price) / current_price) ** (-elasticity) **2\n",
    "        gross_margin -= penalty\n",
    "\n",
    "        return (gross_margin,)\n",
    "    except Exception as e:\n",
    "        print(\"Error in evalGrossMargin:\", e)\n",
    "        return (0,)  # Return a default or zero fitness in case of error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_price_magna = df_magna['precio_neto'].mean()\n",
    "upper_bound_magna = current_price_magna * 1.3\n",
    "lower_bound_magna = current_price_magna * 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolbox_magna.register('attr_float', random.uniform, lower_bound_magna, upper_bound_magna)\n",
    "toolbox_magna.register('evaluate', functools.partial(evalGrossMargin, train_data = X_train_magna, model_type = model_magna, elasticity = price_elasticity_magna, feature_names = features_magna.columns, current_price = current_price_magna))\n",
    "toolbox_magna.register('mate', tools.cxBlend, alpha = 0.5)\n",
    "toolbox_magna.register('mutate', tools.mutGaussian, mu = 0, sigma = 10, indpb = 0.2)\n",
    "toolbox_magna.register('select', tools.selTournament, tournsize = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_magna = toolbox_magna.population(n = 50)\n",
    "\n",
    "NGEN = 40\n",
    "CXPB = 0.5\n",
    "MUTPB = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gen in range(NGEN):\n",
    "    offspring = algorithms.varAnd(population_magna, toolbox_magna, cxpb = CXPB, mutpb = MUTPB)\n",
    "    fits = toolbox_magna.map(toolbox_magna.evaluate, offspring)\n",
    "    print(f\"Generation {gen}: Fitness Values - {fits}\")\n",
    "    for fit, ind in zip(fits, offspring):\n",
    "        ind.fitness.values = fit\n",
    "    population_magna = toolbox_magna.select(offspring, k = len(population_magna))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_individual = tools.selBest(population_magna, k = 1)[0]\n",
    "print(f\"Optimal price: ${top_individual[0]:.2f}\")\n",
    "print(f\"Maximum Gross Margin: ${top_individual.fitness.values[0]:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
